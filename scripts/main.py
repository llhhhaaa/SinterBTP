# Copyright (c) 2026
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#     http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import sys
from pathlib import Path
root_dir = Path(__file__).resolve().parent.parent
if str(root_dir) not in sys.path:
    sys.path.insert(0, str(root_dir))

import matplotlib
matplotlib.use('Agg')  
import logging
import os
import copy
import json
import numpy as np
import torch
import pandas as pd
from typing import Dict, List, Any
from btp.calibrator import OnlineGradientCalibrator
from btp.model_diagnostics import ModelDiagnostics  
from btp.config import TrainConfig, make_timestamp
from btp.data_loader import DataLoader as CustomDataLoader
from btp.preprocessor import DataPreprocessor
from btp.model import build_model
from btp.trainer import Trainer
from btp.metrics import evaluate_quantile_regression
from btp.health_model import HealthModel
from btp.visualizer import Visualizer
from btp.utils import setup_logging

def restore_absolute_values(
    y_scaled: np.ndarray,
    anchor: np.ndarray,
    scaler_y,
    enable_delta: bool
) -> np.ndarray:
    """
    å°†æ¨¡å‹è¾“å‡ºè¿˜åŸä¸ºç»å¯¹ç‰©ç†å€¼ã€‚
    """
    y_scaled = np.asarray(y_scaled)

    # 1) é€†æ ‡å‡†åŒ–
    if y_scaled.ndim == 3:
        B, S, Q = y_scaled.shape
        mean = np.asarray(scaler_y.mean_, dtype=float)
        scale = np.asarray(scaler_y.scale_, dtype=float)
        if mean.size == 1:
            mean = mean.reshape(1, 1, 1)
            scale = scale.reshape(1, 1, 1)
        elif mean.size == S:
            mean = mean.reshape(1, S, 1)
            scale = scale.reshape(1, S, 1)
        y_phys = y_scaled * scale + mean
    elif y_scaled.ndim == 2:
        B, Q = y_scaled.shape
        mean = np.asarray(scaler_y.mean_, dtype=float)
        scale = np.asarray(scaler_y.scale_, dtype=float)
        if mean.size == 1:
            mean = mean.reshape(1, 1)
            scale = scale.reshape(1, 1)
        y_phys = y_scaled * scale + mean
    else:
        raise ValueError(f"Unsupported y_scaled ndim={y_scaled.ndim}")

    # 2) åŠ ä¸Š Delta Anchor
    if enable_delta:
        if anchor is None: return y_phys
        if hasattr(anchor, "values"): anchor = anchor.values
        anchor = np.asarray(anchor)

        if y_phys.ndim == 3:
            if anchor.ndim == 1: anchor = anchor[:, None, None]
            elif anchor.ndim == 2: anchor = anchor[:, :, None]
            return y_phys + anchor
        if anchor.ndim == 1: anchor = anchor[:, None]
        return y_phys + anchor

    return y_phys


def run_cqr_simulation(
    y_pred_abs: np.ndarray,
    y_true_abs: np.ndarray,
    config: TrainConfig,
    sampling_sec: float,
    tag: str,
    fold_dir: str
):
    """ æ‰§è¡Œåœ¨çº¿æ ¡å‡†ï¼šåå·®ä¿®æ­£ + åŒºé—´å®½åº¦è‡ªé€‚åº” """
    if not config.enable_online_cqr:
        return y_pred_abs, np.zeros((len(y_pred_abs), 2)), {}

    logging.info(f"--- [{tag}] æ‰§è¡Œåœ¨çº¿æ ¡å‡† (Bias + Width) ---")
    # â”€â”€ å› æœå»¶è¿Ÿè®¡ç®—ï¼ˆé˜²æ•°æ®æ³„æ¼ï¼‰ â”€â”€
    #
    # æ•°æ®æµè¯´æ˜ï¼š
    #   preprocessor._internal_parallel_build ç”¨ arange(start, end) éå†æ¯ä¸ªåŸå§‹è¡Œç´¢å¼• iï¼Œ
    #   å¯¹æ¯ä¸ª i æ„å»ºä¸€ä¸ªæ ·æœ¬ï¼Œå…¶é¢„æµ‹ç›®æ ‡åœ¨ i + forecast_steps Ã— w_rows å¤„ã€‚
    #   å› æ­¤ç›¸é‚»è¾“å‡ºæ ·æœ¬é—´è· â‰ˆ 1 ä¸ªåŸå§‹è¡Œã€‚
    #
    # å› æœçº¦æŸï¼š
    #   åœ¨æ ¡å‡†æ­¥ tï¼Œè¦ä½¿ç”¨æ­¥ (t - delay) çš„åé¦ˆï¼Œéœ€è¦è¯¥æ­¥çš„çœŸå€¼å·²å¯è§‚æµ‹ï¼š
    #     t_raw >= (t_raw - delay) + forecast_steps
    #   å³ delay >= forecast_steps
    #
    # ä¸å†ä½¿ç”¨ç²’åŒ–çª—å£ï¼Œdelay ç›´æ¥ç­‰äº forecast_steps
    #
    f_steps = int(getattr(config, "forecast_steps", 5))
    prediction_offset = int(getattr(config, "prediction_offset", 0))
    # [ä¿®å¤] CQR å› æœæ€§ï¼šå»¶è¿Ÿè‡³å°‘è¦†ç›– prediction_offset + forecast_steps
    min_delay = prediction_offset + f_steps
    delay_steps = max(min_delay, getattr(config, "delay_steps", 0))
    if delay_steps < min_delay:
        raise ValueError(
            f"[Calibrator] delay_steps({delay_steps}) < prediction_offset + forecast_steps({min_delay})ï¼Œå­˜åœ¨æ•°æ®æ³„æ¼é£é™©"
        )
    logging.info(
        f"[Calibrator] delay_steps={delay_steps} (prediction_offset={prediction_offset}, forecast_steps={f_steps})"
    )
    
    calibrator = OnlineGradientCalibrator(
        bias_window=getattr(config, "cqr_bias_window", 60),
        min_samples=getattr(config, "cqr_min_samples", 20),
    )

    y_calib, corrections, diag_list = calibrator.apply(
        y_pred_abs, 
        y_true_abs, 
        delay_steps=delay_steps
    )
    
    final_diag = diag_list[-1] if diag_list else {}
    if fold_dir:
        os.makedirs(fold_dir, exist_ok=True)
        pd.DataFrame(diag_list).to_csv(os.path.join(fold_dir, "cqr_process_log.csv"), index=False)

    return y_calib, corrections, final_diag

def run_full_pipeline(config: TrainConfig):
    # ========== 0. ç¯å¢ƒä¸æ—¥å¿—è®¾ç½® ==========
    if not config.output_dir:
        timestamp = make_timestamp()
        config.output_dir = os.path.join("outputs", f"run_{timestamp}")

    os.makedirs(config.output_dir, exist_ok=True)
    log_path = os.path.join(config.output_dir, "training.log")
    setup_logging(log_path, reset_handlers=True)

    config.save_json(os.path.join(config.output_dir, "config.json"))
    # [ä¿®å¤] ç»Ÿä¸€è®¾ç½®éšæœºç§å­ï¼Œä¿è¯å¯å¤ç°
    import random
    torch.manual_seed(config.seed)
    np.random.seed(config.seed)
    random.seed(config.seed)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    logging.info("=" * 80)
    logging.info("  ğŸš€ BTP é¢„æµ‹ç³»ç»Ÿ (åŸå§‹é«˜é¢‘åºåˆ—)")
    logging.info("=" * 80)
    logging.info(f"[Config] Device: {device} | Model: {config.model_type} | SeqLen: {config.raw_seq_len}")

    # ========== 1. æ•°æ®åŠ è½½ä¸é¢„å¤„ç† ==========
    data_loader = CustomDataLoader()
    df_raw, sampling_sec = data_loader.load_xlsx(config.excel_path, prefer_time_col="æ—¶é—´")

    preprocessor = DataPreprocessor(config)
    df_feat = preprocessor.build_features(df_raw)

    n_total = len(df_feat)
    n_test_holdout = int(n_total * config.test_split)
    df_cv_pool = df_feat.iloc[:-n_test_holdout].copy() if n_test_holdout > 0 else df_feat.copy()
    df_test_final = df_feat.iloc[-n_test_holdout:].copy() if n_test_holdout > 0 else None

    # æ£€æŸ¥æ˜¯å¦ä»…è¿è¡Œé¢„ç”Ÿæˆé€»è¾‘ (å¯¹äº main æµç¨‹)
    if not config.enable_cv and getattr(config, "PREGENERATE_ONLY", False):
        logging.info("[Mode] PREGENERATE_ONLY ä¸ºçœŸï¼Œæ­£åœ¨é¢„ç”Ÿæˆå¹¶ç¼“å­˜æ•°æ®...")
        # è§¦å‘é¢„å¤„ç†å’Œç¼“å­˜
        _ = preprocessor.process_and_split(df_feat, sampling_sec)
        logging.info("[Mode] æ•°æ®å·²ç”Ÿæˆå¹¶ç¼“å­˜ï¼Œç¨‹åºé€€å‡ºã€‚")
        return

    # [ä¿®å¤] é¢„å¤„ç†å™¨ä¿å­˜åº”åœ¨ process_and_split ä¹‹åï¼Œç¡®ä¿ scaler å·²æ‹Ÿåˆ

    # ==========================================
    # ğŸ”„ Phase 1: æ»šåŠ¨äº¤å‰éªŒè¯ (Rolling CV)
    # ==========================================
    cv_metrics_list: List[Dict] = []
    
    if config.enable_cv:
        cv_root_dir = os.path.join(config.output_dir, "cv_results")
        os.makedirs(cv_root_dir, exist_ok=True)
        
        # æ£€æŸ¥æ˜¯å¦ä»…è¿è¡Œé¢„ç”Ÿæˆé€»è¾‘ (å¯¹äº CV æµç¨‹)
        if getattr(config, "PREGENERATE_ONLY", False):
            logging.info("[Mode] PREGENERATE_ONLY ä¸ºçœŸï¼Œæ­£åœ¨é¢„ç”Ÿæˆå¹¶ç¼“å­˜æ‰€æœ‰ CV Folds æ•°æ®...")
            # è§¦å‘ yield_rolling_folds ä»¥ç”Ÿæˆç¼“å­˜
            for _ in preprocessor.yield_rolling_folds(df_cv_pool, sampling_sec):
                pass
            logging.info("[Mode] æ‰€æœ‰ CV Folds æ•°æ®å·²ç”Ÿæˆå¹¶ç¼“å­˜ï¼Œç¨‹åºé€€å‡ºã€‚")
            return

        # ä½¿ç”¨æ—¶åºç‰ˆäº¤å‰éªŒè¯ï¼ˆå¸¦ç‹¬ç«‹ test é›†ï¼‰
        for fold_idx, data_dict_full in preprocessor.yield_rolling_folds(df_cv_pool, sampling_sec):
            fold_name = f"Fold_{fold_idx + 1}"
            logging.info(f"\n>>> ğŸŸ¢ [CV] Processing {fold_name} (Trainâ†’Valâ†’Test æ—¶åºåˆ’åˆ†)...")
            
            # [å•ä¸€ä»»åŠ¡æ¨¡å¼]
            task_dir = os.path.join(cv_root_dir, fold_name, "Standard")
            os.makedirs(task_dir, exist_ok=True)
            
            cfg_fold = copy.deepcopy(config)
            m_type = getattr(cfg_fold, "model_type", "enhanced_transformer")
            data_dict_task = data_dict_full.copy()
            
            # [æ•°æ®åˆ†å‘] - ç»Ÿä¸€ä½¿ç”¨åŸå§‹åºåˆ— (ç²’åŒ–åŠŸèƒ½å·²ç§»é™¤)
            data_dict_task["X_tr"] = data_dict_full["X_tr_raw"]
            data_dict_task["X_val"] = data_dict_full["X_val_raw"]
            data_dict_task["X_test"] = data_dict_full["X_test_raw"]
            input_dim = data_dict_full["raw_feat_dim"]
            
            # --- æ„å»ºæ¨¡å‹ ---
            model = build_model(
                cfg_fold,
                input_dim=input_dim,
                model_type=m_type
            )
            
            # --- è®­ç»ƒï¼ˆä½¿ç”¨ train æ•°æ®ï¼Œval ç”¨äº early stoppingï¼‰---
            trainer = Trainer(model, cfg_fold, device)
            trainer.train(data_dict_task, verbose=True)
            
            # --- åœ¨ TEST é›†ä¸Šè¯„ä¼°ï¼ˆæœ€ç»ˆç»“æœï¼‰---
            logging.info(f"  ğŸ”® [Predict] åœ¨ç‹¬ç«‹ TEST é›†ä¸Šè¯„ä¼°...")
            X_test_tensor = torch.FloatTensor(data_dict_task["X_test"]).to(device)
            
            # MC Dropout å¤šæ¬¡é‡‡æ ·å¹³å‡
            mc_samples = getattr(cfg_fold, 'mc_samples', 10)
            logging.info(f"  ğŸ”® [MC Dropout] ä½¿ç”¨ {mc_samples} æ¬¡é‡‡æ ·å¹³å‡...")
            preds_test_scaled = trainer.predict(X_test_tensor, mc_samples=mc_samples)
            logging.info(f"  ğŸ”® [MC Dropout] é‡‡æ ·å®Œæˆ")
            
            y_pred_scaled = preds_test_scaled

            y_pred_abs_full = restore_absolute_values(
                y_pred_scaled, data_dict_full["anchor_test"],
                preprocessor.scaler_y, cfg_fold.enable_delta_forecast
            )
            
            # å–æœ€åä¸€æ­¥ T+5
            y_pred_abs_last = y_pred_abs_full[:, -1, :]
            anc = data_dict_full["anchor_test"].reshape(-1, 1) if cfg_fold.enable_delta_forecast else 0
            y_true_abs_full = data_dict_full["y_test_raw"] + anc
            y_true_abs_last = y_true_abs_full[:, -1]
            y_true_abs_tiled = np.tile(y_true_abs_last.reshape(-1, 1), (1, 5))

            # è®¡ç®— MAE (Raw â€” æ— æ ¡å‡†) - ä½¿ç”¨ TEST é›†ç»“æœ
            step_maes = [np.mean(np.abs(y_pred_abs_full[:, s, 2] - y_true_abs_full[:, s])) for s in range(y_pred_abs_full.shape[1])]
            metrics_raw = evaluate_quantile_regression(y_true_abs_tiled, y_pred_abs_last, compute_width_stats=True)
            metrics_raw.update({'mae_avg': np.mean(step_maes), 'fold': fold_idx + 1, 'cqr': 'off', 'split': 'test'})

            # å¥åº·åº¦åˆ†æ (Raw)
            health_model = HealthModel(cfg_fold)
            h_res = health_model.analyze(y_pred_abs_last, y_true=y_true_abs_tiled)
            metrics_raw.update({f"health_{k}": v for k, v in h_res.items() if isinstance(v, (float, int))})

            cv_metrics_list.append(metrics_raw)

            # â”€â”€ CQR æ¶ˆèï¼šåœ¨åŒä¸€ fold ä¸Šè·‘æ ¡å‡†ç‰ˆæœ¬ â”€â”€
            # [åŸºçº¿æ¨¡å‹] è·³è¿‡ CQR æ ¡å‡†
            is_baseline = m_type.startswith("baseline_")
            if getattr(cfg_fold, "enable_cv_cqr", False) and not is_baseline:
                cqr_dir = os.path.join(task_dir, "cqr_ablation")
                y_val_calib, val_corr, cqr_diag = run_cqr_simulation(
                    y_pred_abs_last, y_true_abs_tiled, cfg_fold, sampling_sec,
                    f"CV_F{fold_idx+1}", cqr_dir
                )
                metrics_cqr = evaluate_quantile_regression(y_true_abs_tiled, y_val_calib, compute_width_stats=True)
                step_maes_cqr = [np.mean(np.abs(y_pred_abs_full[:, s, 2] - y_true_abs_full[:, s])) for s in range(y_pred_abs_full.shape[1])]
                metrics_cqr.update({'mae_avg': np.mean(step_maes_cqr), 'fold': fold_idx + 1, 'cqr': 'on'})
                # CQR æ ¡å‡†åçš„ Q50 MAE
                metrics_cqr['mae_q50_calib'] = float(np.mean(np.abs(y_val_calib[:, 2] - y_true_abs_tiled[:, 2])))
                h_res_cqr = health_model.analyze(y_val_calib, y_true=y_true_abs_tiled)
                metrics_cqr.update({f"health_{k}": v for k, v in h_res_cqr.items() if isinstance(v, (float, int))})
                cv_metrics_list.append(metrics_cqr)
                logging.info(f"  [CQR æ¶ˆè] Fold {fold_idx+1}: "
                             f"Raw MAE={metrics_raw.get('mae_q50', 0):.4f} â†’ CQR MAE={metrics_cqr['mae_q50_calib']:.4f}")

                # CQR æ¶ˆèå¯è§†åŒ–
                vis_cqr_dir = os.path.join(task_dir, "visualizations_cqr")
                vis_cqr = Visualizer(vis_cqr_dir, cfg_fold)
                vis_cqr.generate_all_plots(y_val_calib, y_true_abs_tiled, cfg_fold,
                                           y_raw=y_pred_abs_last, corrections=val_corr,
                                           health_res=h_res_cqr, prefix="cqr")

            # å¯è§†åŒ– (Raw â€” æ— æ ¡å‡†)
            vis_dir = os.path.join(task_dir, "visualizations")
            visualizer = Visualizer(vis_dir, cfg_fold)
            visualizer.generate_all_plots(y_pred_abs_last, y_true_abs_tiled, cfg_fold, y_raw=y_pred_abs_last,
                                        corrections=np.zeros_like(y_pred_abs_last[:,:2]), health_res=h_res)
            
            # ä¿å­˜æ®‹å·®
            diag_dir = os.path.join(task_dir, "diagnostics")
            os.makedirs(diag_dir, exist_ok=True)
            val_timestamps = df_cv_pool["æ—¶é—´"].iloc[-len(y_pred_abs_last):].values
            
            df_resid = pd.DataFrame({
                'Timestamp': val_timestamps,
                'True_Value': y_true_abs_tiled[:, 2], 
                'Pred_Value': y_pred_abs_last[:, 2],  
                'Residual': y_true_abs_tiled[:, 2] - y_pred_abs_last[:, 2]
            })
            df_resid.to_csv(os.path.join(diag_dir, "residual_analysis.csv"), index=False)

            # è¯Šæ–­ä¸é²æ£’æ€§
            diagnoser = ModelDiagnostics(diag_dir, cfg_fold)
            robust_df = diagnoser.perform_robustness_test(
                model=model,
                X_test=X_test_tensor,
                y_true_abs=y_true_abs_tiled,
                trainer=trainer,
                preprocessor=preprocessor,
                anchor=data_dict_task.get("anchor_val"),
                enable_delta=cfg_fold.enable_delta_forecast
            )
            metrics_raw['robustness_score'] = robust_df["æ€§èƒ½ä¿æŒç‡"].iloc[-1] 

        # æ±‡æ€» CV ç»“æœï¼ˆåªæŠ¥å‘Š TEST é›†ç»“æœï¼‰
        summary_path = os.path.join(cv_root_dir, "cv_summary_report.csv")
        df_cv_summary = pd.DataFrame(cv_metrics_list)
        df_cv_summary.to_csv(summary_path, index=False)
        
        # è®¡ç®— TEST é›†ç»“æœçš„å‡å€¼Â±æ ‡å‡†å·®
        test_metrics = df_cv_summary[df_cv_summary['split'] == 'test'] if 'split' in df_cv_summary.columns else df_cv_summary
        if len(test_metrics) > 0:
            logging.info("\n" + "=" * 60)
            logging.info("  ğŸ“Š [CV æ±‡æ€»] TEST é›†ç»“æœ (å‡å€¼ Â± æ ‡å‡†å·®)")
            logging.info("=" * 60)
            for col in ['mae_avg', 'mae_q50', 'coverage_80', 'coverage_50', 'interval_width_mean']:
                if col in test_metrics.columns:
                    mean_val = test_metrics[col].mean()
                    std_val = test_metrics[col].std()
                    logging.info(f"  {col}: {mean_val:.4f} Â± {std_val:.4f}")
            logging.info("=" * 60)

    # ==========================================
    # ğŸ§ª Phase 2: æœ€ç»ˆå…¨é‡æµ‹è¯• (Refit)
    # ==========================================
    if df_test_final is not None:
        logging.info("\n" + "=" * 80 + "\n  ğŸ§ª æœ€ç»ˆæµ‹è¯•é˜¶æ®µ (Refit)")
        final_test_root = os.path.join(config.output_dir, "final_test_results")
        os.makedirs(final_test_root, exist_ok=True)

        df_full = pd.concat([df_cv_pool, df_test_final])
        final_data = preprocessor.process_and_split(df_full, sampling_sec)
        preprocessor.save(os.path.join(config.output_dir, "preprocessor_meta"))
        
        m_type = getattr(config, "model_type", "enhanced_transformer")
        train_package = final_data.copy()
        
        # ç»Ÿä¸€ä½¿ç”¨åŸå§‹åºåˆ— (ç²’åŒ–åŠŸèƒ½å·²ç§»é™¤)
        train_package["X_tr"] = final_data["X_tr_raw"]
        train_package["X_val"] = final_data["X_val_raw"]
        X_test_input = final_data["X_test_raw"]
        input_dim = final_data["raw_feat_dim"]

        final_model = build_model(config, input_dim=input_dim, model_type=m_type)
        final_trainer = Trainer(final_model, config, device)
        final_trainer.train(train_package, verbose=True)

        # MC Dropout å¤šæ¬¡é‡‡æ ·å¹³å‡ (æœ€ç»ˆæµ‹è¯•)
        mc_samples = getattr(config, 'mc_samples', 30)
        preds_test_scaled = final_trainer.predict(torch.FloatTensor(X_test_input), mc_samples=mc_samples)
        
        y_test_pred_abs_full = restore_absolute_values(preds_test_scaled, final_data["anchor_test"], preprocessor.scaler_y, config.enable_delta_forecast)
        
        y_test_pred_abs_last = y_test_pred_abs_full[:, -1, :]
        anc_test = final_data["anchor_test"].reshape(-1, 1) if config.enable_delta_forecast else 0
        y_test_true_abs_full = final_data["y_test_raw"] + anc_test
        y_test_true_tiled = np.tile(y_test_true_abs_full[:, -1].reshape(-1, 1), (1, 5))

        # [åŸºçº¿æ¨¡å‹] è·³è¿‡ CQR æ ¡å‡†ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹é¢„æµ‹
        is_baseline = m_type.startswith("baseline_")
        if is_baseline:
            logging.info("  [Baseline] è·³è¿‡ CQR æ ¡å‡† (åŸºçº¿æ¨¡å‹ä¸ä½¿ç”¨ CQR)")
            y_test_calib = y_test_pred_abs_last
            test_corr = np.zeros_like(y_test_pred_abs_last[:, :2])
        else:
            y_test_calib, test_corr, _ = run_cqr_simulation(y_test_pred_abs_last, y_test_true_tiled, config, sampling_sec, "Final", final_test_root)

        # æœ€ç»ˆç»˜å›¾ä¸ä¿å­˜
        vis_dir = os.path.join(final_test_root, "visualizations")
        vis_final = Visualizer(vis_dir, config)

        # å¥åº·åº¦è®¡ç®—ç»“æœ (å¦‚æœéœ€è¦ä¼ å…¥ï¼Œéœ€è¦å…ˆè®¡ç®—)
        # è¿™é‡Œå‡è®¾ y_test_calib å·²ç»æ˜¯æœ€ç»ˆè¾“å‡ºï¼Œæˆ‘ä»¬éœ€è¦é‡æ–°è®¡ç®—å¥åº·åº¦ä»¥è·å– health_results
        health_model = HealthModel(config)
        health_results = health_model.analyze(y_test_calib, y_true=y_test_true_tiled)

        vis_final.generate_all_plots(
            y_pred=y_test_calib,
            y_true=y_test_true_tiled,
            config=config,
            y_raw=y_test_pred_abs_last,
            corrections=test_corr,
            history=getattr(final_trainer, "history", None),
            y_obs_win=final_data.get("y_test_obs", None), # å°è¯•è·å–è§‚æµ‹çª—ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä¸º None
            health_res=health_results,
            prefix="final_test"
        )
        
        # 2. [æ–°å¢] ä¸“é—¨è°ƒç”¨æ··æ·†çŸ©é˜µç»˜åˆ¶
        if 'true_states' in health_results:
            vis_final.plot_diagnosis_confusion_matrix(
                y_true_states=health_results['true_states'],
                y_pred_states=health_results['pred_states'],
                fname="final_test_diagnosis_confusion_matrix.png"
            )

        # ==========================================
        # [æ–°å¢] 1. å¥åº·åº¦ç›¸å…³æ€§åˆ†ææ—¥å¿—ä¸ç»˜å›¾
        # ==========================================
        if 'true_health_scores' in health_results:
            h_true = health_results['true_health_scores']
            h_pred = health_results['health_scores']
            
            # è®¡ç®— Pearson ç›¸å…³ç³»æ•°
            if len(h_true) > 1:
                corr_val = np.corrcoef(h_true, h_pred)[0, 1]
                # metrics_final['health_correlation'] = corr_val # metrics_final æœªå®šä¹‰ï¼Œè·³è¿‡
                logging.info(f"  å¥åº·åº¦é¢„æµ‹ç›¸å…³ç³»æ•° (Pearson R): {corr_val:.4f}")
                
                # ç»˜åˆ¶ç›¸å…³æ€§åˆ†æå›¾
                vis_final.plot_health_correlation(
                    health_res=health_results,
                    fname="final_test_health_correlation.png"
                )
            
            # [æ–°å¢] 2. ç»˜åˆ¶ BTP + Health ä¸Šä¸‹å¯¹é½å…¨æ™¯å›¾
            # éœ€è¦ä¼ å…¥ mu å‚æ•°ä¾›ç”»å‚è€ƒçº¿ï¼Œä» config è·å–
            mu_val = getattr(config, 'health_mu', 22.6)
            vis_final.plot_btp_health_panorama(
                y_true=y_test_true_tiled,
                y_pred=y_test_calib,
                health_res=health_results,
                fname="final_test_btp_health_panorama.png",
                mu=mu_val
            )
            logging.info("  å·²ç”Ÿæˆ BTP+å¥åº·åº¦å…¨æ™¯å¯¹æ¯”å›¾ (final_test_btp_health_panorama.png)")

        # ==========================================
        # ğŸ›¡ï¸ Step 11: Model Diagnostics (æ·±åº¦éªŒè¯ - åˆ†é’Ÿçº§)
        # ==========================================
        diag_dir = os.path.join(final_test_root, "diagnostics")
        diagnoser = ModelDiagnostics(diag_dir, config)
        
        # [ä¿®æ”¹ç‚¹] æå–æµ‹è¯•é›†å¯¹åº”çš„æ—¶é—´æˆ³
        # æˆ‘ä»¬éœ€è¦æœ€å N_test ä¸ªæ ·æœ¬çš„æ—¶é—´æˆ³ï¼Œå¹¶è·³è¿‡é¢„å¤„ç†çš„ buffer
        test_timestamps = df_full["æ—¶é—´"].iloc[-len(y_test_calib):].values
        
        # 1. æ‰§è¡Œæ®‹å·®åˆ†æ (ä¼ å…¥æ—¶é—´æˆ³)
        res_info = diagnoser.perform_residual_analysis(
            y_test_true_tiled,
            y_test_calib,
            timestamps=test_timestamps
        )
        vis_final.plot_residual_diagnostic(res_info, "final_residual_analysis_1min.png")
        

        # 2. æ‰§è¡Œé²æ£’æ€§å‹åŠ›æµ‹è¯• (ä¼ å…¥ Anchor ä»¥è¿˜åŸç‰©ç†å€¼)
        # ç»Ÿä¸€ä½¿ç”¨åŸå§‹åºåˆ— (ç²’åŒ–åŠŸèƒ½å·²ç§»é™¤)
        X_test_input = final_data["X_test_raw"]
        
        robust_df = diagnoser.perform_robustness_test(
            final_model,
            torch.FloatTensor(X_test_input).to(device), # è½¬æ¢ä¸º Tensor å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
            y_test_true_tiled,
            final_trainer,
            preprocessor,
            anchor=final_data["anchor_test"],           # <--- æ–°å¢
            enable_delta=config.enable_delta_forecast   # <--- æ–°å¢
        )
        vis_final.plot_robustness_stress_test(robust_df, "final_robustness_test.png")
        
        # 3. å¯¼å‡º Excel
        diagnoser.export_to_excel()
        
        logging.info("ğŸ›¡ï¸ [Diagnostics] æ®‹å·®åˆ†æä¸é²æ£’æ€§æµ‹è¯•å·²å®Œæˆã€‚")

        logging.info("\n" + "=" * 80 + "\n  âœ… æœ€ç»ˆæµ‹è¯•å®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³: " + final_test_root + "\n" + "=" * 80)


def run_ablation_study(base_config: TrainConfig = None):
    """
    è¿è¡Œæ¶ˆèå®éªŒï¼Œå¯¹æ¯”å„æ¨¡å—çš„è´¡çŒ®
    
    Args:
        base_config: åŸºç¡€é…ç½®ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨é»˜è®¤é…ç½®
    """
    if base_config is None:
        base_config = TrainConfig(
            excel_path="20130503ycz.xlsx",
            target_column="åŒ—ä¾§_è®¡ç®—BTPä½ç½®",
        )
    
    timestamp = make_timestamp()
    ablation_root = os.path.join("outputs", f"Ablation_Study_{timestamp}")
    os.makedirs(ablation_root, exist_ok=True)
    
    # å®šä¹‰æ¶ˆèé…ç½®
    # [ä¿®å¤] æ¶ˆèä¸»ä½“ç»Ÿä¸€å…³é—­ CQRï¼Œä¿è¯ç»“æ„å¯¹æ¯”å…¬å¹³
    ablation_configs = [
        {"name": "full_model_no_cqr", "changes": {"enable_online_cqr": False}},
        {"name": "no_positional_encoding", "changes": {"enable_positional_encoding": False, "enable_online_cqr": False}},
        {"name": "no_mc_dropout", "changes": {"enable_mc_dropout": False, "enable_online_cqr": False}},
        {"name": "no_revin", "changes": {"enable_revin": False, "enable_online_cqr": False}},
        {"name": "layers_1", "changes": {"num_transformer_layers": 1, "enable_online_cqr": False}},
        {"name": "layers_3", "changes": {"num_transformer_layers": 3, "enable_online_cqr": False}},
        {"name": "heads_2", "changes": {"attn_heads": 2, "enable_online_cqr": False}},
        {"name": "heads_8", "changes": {"attn_heads": 8, "enable_online_cqr": False}},
        # [æ–°å¢å¯¹ç…§] å®Œæ•´æ¨¡å‹ + CQR
        {"name": "full_model_with_cqr", "changes": {"enable_online_cqr": True}},
    ]
    
    print("\n" + "=" * 70)
    print(f"[Ablation Study] å¯åŠ¨æ¶ˆèå®éªŒ")
    print(f"æ€»è¾“å‡ºç›®å½•: {ablation_root}")
    print(f"æ¶ˆèé…ç½®æ•°é‡: {len(ablation_configs)}")
    print("=" * 70 + "\n")
    
    # å­˜å‚¨æ‰€æœ‰å®éªŒç»“æœ
    all_results = []
    
    # å¯¹æ¯ä¸ªé…ç½®è¿è¡Œäº¤å‰éªŒè¯
    for i, ablation_cfg in enumerate(ablation_configs):
        exp_name = ablation_cfg["name"]
        changes = ablation_cfg["changes"]
        
        print(f"\n>>> [{i+1}/{len(ablation_configs)}] è¿è¡Œæ¶ˆèé…ç½®: {exp_name}")
        if changes:
            print(f"    ä¿®æ”¹å‚æ•°: {changes}")
        else:
            print(f"    (å®Œæ•´æ¨¡å‹åŸºå‡†)")
        
        # åˆ›å»ºå½“å‰é…ç½®çš„å‰¯æœ¬å¹¶åº”ç”¨ä¿®æ”¹
        current_cfg = copy.deepcopy(base_config)
        for key, value in changes.items():
            if hasattr(current_cfg, key):
                setattr(current_cfg, key, value)
            else:
                print(f"    [Warning] é…ç½®é¡¹ {key} ä¸å­˜åœ¨ï¼Œè·³è¿‡")
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        current_cfg.output_dir = os.path.join(ablation_root, f"Ablation_{exp_name}")
        current_cfg.exp_name = f"Ablation_{exp_name}"
        
        try:
            run_full_pipeline(current_cfg)
            print(f"    [Success] {exp_name} è¿è¡ŒæˆåŠŸ")
            
            # å°è¯•è¯»å–ç»“æœ
            result_path = os.path.join(current_cfg.output_dir, "analysis_report.xlsx")
            if os.path.exists(result_path):
                try:
                    df_result = pd.read_excel(result_path, sheet_name="CV_Summary")
                    all_results.append({
                        "name": exp_name,
                        "changes": str(changes),
                        "mae_mean": df_result["MAE"].mean() if "MAE" in df_result.columns else None,
                        "rmse_mean": df_result["RMSE"].mean() if "RMSE" in df_result.columns else None,
                        "coverage_mean": df_result["Coverage_80"].mean() if "Coverage_80" in df_result.columns else None,
                    })
                except Exception as e:
                    print(f"    [Warning] è¯»å–ç»“æœå¤±è´¥: {e}")
                    all_results.append({"name": exp_name, "changes": str(changes), "error": str(e)})
            
        except Exception as e:
            print(f"    [Failed] {exp_name} è¿è¡Œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            all_results.append({"name": exp_name, "changes": str(changes), "error": str(e)})
            continue
    
    # æ±‡æ€»æ¶ˆèå®éªŒç»“æœ
    print("\n" + "=" * 70)
    print("[Ablation Study] æ¶ˆèå®éªŒç»“æœæ±‡æ€»")
    print("=" * 70)
    
    if all_results:
        summary_df = pd.DataFrame(all_results)
        summary_path = os.path.join(ablation_root, "ablation_summary.xlsx")
        summary_df.to_excel(summary_path, index=False)
        print(f"\nç»“æœå·²ä¿å­˜è‡³: {summary_path}")
        
        # æ‰“å°ç»“æœè¡¨æ ¼
        print("\n" + "-" * 70)
        print(f"{'é…ç½®åç§°':<30} {'MAE':<12} {'RMSE':<12} {'Coverage':<12}")
        print("-" * 70)
        for r in all_results:
            if "error" not in r:
                mae = f"{r.get('mae_mean', 'N/A'):.4f}" if r.get('mae_mean') is not None else "N/A"
                rmse = f"{r.get('rmse_mean', 'N/A'):.4f}" if r.get('rmse_mean') is not None else "N/A"
                cov = f"{r.get('coverage_mean', 'N/A'):.2%}" if r.get('coverage_mean') is not None else "N/A"
                print(f"{r['name']:<30} {mae:<12} {rmse:<12} {cov:<12}")
            else:
                print(f"{r['name']:<30} {'ERROR':<12} {r.get('error', '')[:30]}")
        print("-" * 70)
    
    print("\n" + "=" * 70)
    print("[Ablation Study] æ¶ˆèå®éªŒå®Œæˆï¼")
    print(f"è¾“å‡ºç›®å½•: {ablation_root}")
    print("=" * 70 + "\n")
    
    return all_results


def run_model_diagnostics(base_config: TrainConfig):
    """
    è¿è¡Œ Model Diagnostics æ·±åº¦éªŒè¯å®éªŒ
    åŒ…å«ï¼šæ®‹å·®åˆ†æã€é²æ£’æ€§æµ‹è¯•ã€è¶…å‚æ•°æ•æ„Ÿæ€§åˆ†æ
    """
    from btp.model_diagnostics import ModelDiagnostics
    
    timestamp = make_timestamp()
    
    # ä½¿ç”¨é…ç½®ä¸­æŒ‡å®šçš„è¾“å‡ºç›®å½•
    diagnostics_root = base_config.diagnostics_output_dir
    output_dir = os.path.join(diagnostics_root, f"Diagnostics_{timestamp}")
    os.makedirs(output_dir, exist_ok=True)
    
    print("\n" + "="*70)
    print("[Model Diagnostics] å¯åŠ¨æ·±åº¦éªŒè¯å®éªŒ")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print("="*70 + "\n")
    
    # ä¿å­˜é…ç½®
    base_config.save_json(os.path.join(output_dir, "config.json"))
    
    # è®¾ç½®æ—¥å¿—
    log_file = os.path.join(output_dir, "diagnostics.log")
    setup_logging(log_file)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logging.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # 1. åŠ è½½æ•°æ®
    print("[1/5] åŠ è½½æ•°æ®...")
    loader = CustomDataLoader()
    df_raw, sampling_sec = loader.load_xlsx(base_config.excel_path, prefer_time_col="æ—¶é—´")
    
    # 2. æ•°æ®é¢„å¤„ç†
    print("[2/5] æ•°æ®é¢„å¤„ç†...")
    preprocessor = DataPreprocessor(base_config)
    df_feat = preprocessor.build_features(df_raw)
    
    # ä½¿ç”¨ process_and_split è·å–è®­ç»ƒ/éªŒè¯/æµ‹è¯•æ•°æ®
    data_dict = preprocessor.process_and_split(df_feat, sampling_sec)
    
    # æå–æ•°æ®
    X_train = data_dict["X_tr_raw"]
    X_val = data_dict["X_val_raw"]
    X_test = data_dict["X_test_raw"]
    y_train = data_dict["y_tr"]
    y_val = data_dict["y_val"]
    y_test = data_dict["y_test"]
    anchor_test = data_dict["anchor_test"]
    input_dim = data_dict["raw_feat_dim"]
    
    # è½¬æ¢ä¸º Tensor
    X_train_t = torch.tensor(X_train, dtype=torch.float32)
    X_val_t = torch.tensor(X_val, dtype=torch.float32)
    X_test_t = torch.tensor(X_test, dtype=torch.float32)
    
    # 3. æ„å»ºå¹¶è®­ç»ƒåŸºå‡†æ¨¡å‹
    print("[3/5] è®­ç»ƒåŸºå‡†æ¨¡å‹...")
    model = build_model(base_config, input_dim)
    trainer = Trainer(model, base_config, device)
    
    # è®­ç»ƒæ¨¡å‹ (ä½¿ç”¨ data_dict æ ¼å¼)
    train_package = {
        "X_tr": X_train,
        "X_val": X_val,
        "y_tr": y_train,
        "y_val": y_val
    }
    trainer.train(train_package, verbose=True)
    
    # 4. è·å–é¢„æµ‹ç»“æœ
    print("[4/5] ç”Ÿæˆé¢„æµ‹ç»“æœ...")
    y_pred_scaled = trainer.predict(X_test_t, mc_samples=base_config.mc_samples)
    
    # è¿˜åŸç»å¯¹å€¼
    y_pred_abs = restore_absolute_values(
        y_pred_scaled,
        anchor_test,
        preprocessor.scaler_y,
        base_config.enable_delta_forecast
    )
    
    # å¤„ç† y_pred_abs å½¢çŠ¶: (N, 1, 5) -> (N, 5)
    if y_pred_abs.ndim == 3 and y_pred_abs.shape[1] == 1:
        y_pred_abs = y_pred_abs.squeeze(axis=1)
    
    # è¿˜åŸçœŸå€¼
    y_test_last = y_test[:, -1] if y_test.ndim == 2 else y_test
    y_true_abs = restore_absolute_values(
        y_test_last.reshape(-1, 1) if y_test_last.ndim == 1 else y_test_last,
        anchor_test,
        preprocessor.scaler_y,
        base_config.enable_delta_forecast
    )
    
    # æ‰©å±•ä¸º 5 åˆ†ä½æ•°æ ¼å¼ (å¦‚æœéœ€è¦)
    if y_true_abs.ndim == 1:
        y_true_abs = np.tile(y_true_abs.reshape(-1, 1), (1, 5))
    elif y_true_abs.shape[1] == 1:
        y_true_abs = np.tile(y_true_abs, (1, 5))
    
    # 5. è¿è¡Œè¯Šæ–­åˆ†æ
    print("[5/5] è¿è¡Œè¯Šæ–­åˆ†æ...")
    diagnostics = ModelDiagnostics(output_dir, base_config)
    
    # è·å–æ—¶é—´æˆ³
    timestamps = data_dict.get('timestamps_test', None)
    
    # 5.1 æ®‹å·®åˆ†æ
    if base_config.enable_residual_analysis:
        print("  [5.1] æ®‹å·®åˆ†æ...")
        diagnostics.perform_residual_analysis(y_true_abs, y_pred_abs, timestamps)
    
    # 5.2 é²æ£’æ€§æµ‹è¯•
    if base_config.enable_robustness_test:
        print("  [5.2] é²æ£’æ€§æµ‹è¯•...")
        diagnostics.perform_robustness_test(
            model, X_test_t, y_true_abs, trainer, preprocessor,
            anchor=anchor_test, enable_delta=base_config.enable_delta_forecast
        )
    
    # 5.3 è¶…å‚æ•°æ•æ„Ÿæ€§åˆ†æ
    if base_config.enable_hyperparameter_sensitivity:
        print("  [5.3] è¶…å‚æ•°æ•æ„Ÿæ€§åˆ†æ...")
        diagnostics.perform_hyperparameter_sensitivity(
            X_train_t, y_train, X_val_t, y_val,
            build_model, preprocessor, device
        )
    
    # 6. å¯¼å‡ºæŠ¥å‘Š
    print("\n[Export] å¯¼å‡ºè¯Šæ–­æŠ¥å‘Š...")
    report_path = diagnostics.export_to_excel()
    
    print("\n" + "="*70)
    print("[Model Diagnostics] æ·±åº¦éªŒè¯å®éªŒå®Œæˆï¼")
    print(f"æŠ¥å‘Šè·¯å¾„: {report_path}")
    print("="*70 + "\n")
    
    return diagnostics.results


def main():
    # 1. åŸºç¡€é…ç½®åˆå§‹åŒ–
    base_config = TrainConfig(
        excel_path="data/raw/20130503ycz.xlsx",
        target_column="åŒ—ä¾§_è®¡ç®—BTPä½ç½®",
    )
    
    # ==========================================
    # å®éªŒæ¨¡å¼åˆ†å‘
    # ==========================================
    
    # ä¼˜å…ˆçº§ 1: Model Diagnostics æ·±åº¦éªŒè¯å®éªŒ
    if base_config.enable_model_diagnostics:
        print("[Mode] Model Diagnostics æ·±åº¦éªŒè¯æ¨¡å¼å·²å¯ç”¨")
        run_model_diagnostics(base_config)
        return
    
    # ä¼˜å…ˆçº§ 2: æ¶ˆèå®éªŒæ¨¡å¼
    if base_config.enable_ablation_study:
        print("[Mode] æ¶ˆèå®éªŒæ¨¡å¼å·²å¯ç”¨")
        from scripts.run_ablation_experiment import run_ablation_experiment
        run_ablation_experiment(base_config)
        return
    
    # ä¼˜å…ˆçº§ 3: æ¨¡å‹å¯¹æ¯”å®éªŒæ¨¡å¼
    if base_config.enable_model_comparison:
        print("[Mode] æ¨¡å‹å¯¹æ¯”å®éªŒæ¨¡å¼å·²å¯ç”¨")
        # ç»§ç»­æ‰§è¡Œä¸‹é¢çš„æ¨¡å‹å¯¹æ¯”é€»è¾‘
    else:
        # é»˜è®¤ï¼šå•æ¨¡å‹è®­ç»ƒ
        print("[Mode] å•æ¨¡å‹è®­ç»ƒæ¨¡å¼")
        timestamp = make_timestamp()
        base_config.output_dir = os.path.join("data", f"run_{timestamp}")
        base_config.exp_name = f"run_{timestamp}"
        run_full_pipeline(base_config)
        return
    
    # ==========================================
    # æ¨¡å‹å¯¹æ¯”å®éªŒé€»è¾‘
    # ==========================================
    timestamp = make_timestamp()
    experiment_root = os.path.join("outputs", f"Experiment_Significance_{timestamp}")
    os.makedirs(experiment_root, exist_ok=True)
    
    models_to_compare = base_config.comparison_models
    # ç¡®ä¿ä¸»æ¨¡å‹åœ¨å¯¹æ¯”åˆ—è¡¨ä¸­
    main_model = base_config.model_type
    if main_model not in models_to_compare:
        models_to_compare.insert(0, main_model)

    print("\n" + "*"*60)
    print(f"[Start] å¯åŠ¨å•å°ºåº¦å…¨è‡ªåŠ¨ç»Ÿè®¡æ˜¾è‘—æ€§å®éªŒæµæ°´çº¿")
    print(f"æ€»è¾“å‡ºç›®å½•: {experiment_root}")
    print("*"*60 + "\n")

    # å¾ªç¯è®­ç»ƒæ‰€æœ‰æ¨¡å‹
    for m_name in models_to_compare:
        print(f"\n>>> [é˜¶æ®µ 1/2] æ­£åœ¨è¿è¡Œæ¨¡å‹: {m_name.upper()}")
        
        current_cfg = copy.deepcopy(base_config)
        current_cfg.model_type = m_name
        current_cfg.output_dir = os.path.join(experiment_root, f"Compare_{m_name}")
        current_cfg.exp_name = f"Compare_{m_name}"
        
        try:
            run_full_pipeline(current_cfg)
            print(f"[Success] {m_name} è¿è¡ŒæˆåŠŸã€‚")
        except Exception as e:
            print(f"[Failed] {m_name} è¿è¡Œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            continue

    # è·¨æ¨¡å‹ç»Ÿè®¡åˆ†æ
    print("\n" + "*"*60)
    print("[Stats] [é˜¶æ®µ 2/2] å¯åŠ¨ç»Ÿè®¡æ£€éªŒä¸è·¨æ¨¡å‹å¯è§†åŒ–")
    print("*"*60)

    try:
        from btp.stats import perform_significance_test
        stats_report = perform_significance_test(experiment_root, target_model=base_config.model_type)
        
        from btp.visualizer import Visualizer
        summary_viz = Visualizer(save_dir=experiment_root, config=base_config)
        summary_viz.plot_model_comparison_boxplots(experiment_root, fname="final_model_comparison_boxplots.png")
        
        print(f"\n[Info] å®éªŒæŠ¥å‘Šå·²ç”Ÿæˆï¼š\nDirectory: {experiment_root}")
        
    except Exception as e:
        print(f"[Warning] ç»Ÿè®¡æ±‡æ€»é˜¶æ®µå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

    print("\n" + "*"*60)
    print("[Finish] å…¨éƒ¨å®éªŒä»»åŠ¡ç»“æŸï¼")
    print("*"*60)

if __name__ == "__main__":
    main()
