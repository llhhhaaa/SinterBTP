# Copyright (c) 2026
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#     http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import logging
import os
from typing import Dict

# [Modified] Import the correctly named losses
from btp.model import QuantileLoss

class Trainer:
    def __init__(self, model: nn.Module, cfg, device: torch.device):
        self.model = model.to(device)
        self.cfg = cfg
        self.device = device
        # [ä¿®å¤] è®­ç»ƒé…ç½®ä¸ä¸€è‡´ï¼šåŠ å…¥ weight_decay
        self.optimizer = optim.Adam(
            self.model.parameters(),
            lr=float(cfg.lr),
            weight_decay=float(getattr(cfg, "weight_decay", 0.0))
        )
        self.criterion = QuantileLoss(cfg).to(device)
        self.scaler = torch.cuda.amp.GradScaler(enabled=(device.type == 'cuda'))
        self.history = {"train_loss": [], "val_loss": []}
        # [æ–°å¢] ç”¨äºå­˜å‚¨è¯¥æ¨¡å‹åœ¨å½“å‰ Fold ä¸Šçš„åŸå§‹æ®‹å·®åºåˆ—
        self.last_residuals = None
        # [è¯Šæ–­] å­˜å‚¨æ¯ä¸ª epoch çš„è¯Šæ–­æ•°æ®
        self.diagnostics_history = []

    def _should_diagnose(self, epoch: int) -> bool:
        """åˆ¤æ–­æ˜¯å¦åœ¨å½“å‰ epoch è¾“å‡ºè¯Šæ–­ä¿¡æ¯"""
        # ç¬¬ 1, 2, 3, 5, 10 ä¸ª epochï¼Œä¹‹åæ¯ 10 ä¸ª epoch
        return epoch in [0, 1, 2, 4, 9] or (epoch + 1) % 10 == 0

    @torch.no_grad()
    def _compute_diagnostics(self, X: torch.Tensor, y: torch.Tensor, 
                              preds: torch.Tensor, epoch: int, split: str) -> dict:
        """
        è®¡ç®—è®­ç»ƒè¯Šæ–­æŒ‡æ ‡ï¼Œå¸®åŠ©å®šä½åˆ†å¸ƒæ¼‚ç§»å’Œè®­ç»ƒé—®é¢˜ã€‚
        
        è¯Šæ–­ç»´åº¦:
        1. è¾“å…¥åˆ†å¸ƒ: X çš„å‡å€¼/æ ‡å‡†å·® (æ£€æµ‹è¾“å…¥ç«¯æ¼‚ç§»)
        2. è¾“å‡ºåˆ†å¸ƒ: pred vs true çš„ç»Ÿè®¡é‡ (æ£€æµ‹é¢„æµ‹åå·®)
        3. æ®‹å·®åˆ†æ: å‡å€¼/æ ‡å‡†å·®/ååº¦ (æ£€æµ‹ç³»ç»Ÿæ€§åç§»)
        4. åŒºé—´å®½åº¦: Q90-Q10 (æ£€æµ‹åŒºé—´åç¼©)
        5. æ¢¯åº¦èŒƒæ•°: å„å±‚æ¢¯åº¦å¤§å° (æ£€æµ‹æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸)
        """
        diag = {"epoch": epoch + 1, "split": split}
        
        # --- 1. è¾“å…¥åˆ†å¸ƒ ---
        x_mean = X.mean().item()
        x_std = X.std().item()
        # ç›®æ ‡ç‰¹å¾ (ç¬¬0ç»´) çš„ç»Ÿè®¡é‡
        x_target_mean = X[:, -1, 0].mean().item()
        x_target_std = X[:, -1, 0].std().item()
        diag["x_mean"] = round(x_mean, 5)
        diag["x_std"] = round(x_std, 5)
        diag["x_target_last_mean"] = round(x_target_mean, 5)
        diag["x_target_last_std"] = round(x_target_std, 5)
        
        # --- 2. è¾“å‡ºåˆ†å¸ƒ: pred Q50 vs true ---
        pred_q50 = preds[..., 2]  # (B, Steps)
        true_vals = y if y.ndim == 2 else y.squeeze(-1)  # (B, Steps)
        
        # æœ€ç»ˆæ­¥ (forecast_steps çš„æœ€åä¸€æ­¥ï¼Œæœ€é‡è¦)
        pred_last = pred_q50[:, -1]
        true_last = true_vals[:, -1]
        
        diag["pred_q50_mean"] = round(pred_q50.mean().item(), 5)
        diag["pred_q50_std"] = round(pred_q50.std().item(), 5)
        diag["true_mean"] = round(true_vals.mean().item(), 5)
        diag["true_std"] = round(true_vals.std().item(), 5)
        diag["pred_last_mean"] = round(pred_last.mean().item(), 5)
        diag["true_last_mean"] = round(true_last.mean().item(), 5)
        
        # --- 3. æ®‹å·®åˆ†æ ---
        residuals = true_vals - pred_q50  # (B, Steps)
        res_mean = residuals.mean().item()
        res_std = residuals.std().item()
        res_abs_mean = residuals.abs().mean().item()
        
        # ååº¦ (skewness) â€” æ£€æµ‹ç³»ç»Ÿæ€§åç§»æ–¹å‘
        if res_std > 1e-8:
            res_skew = ((residuals - res_mean) ** 3).mean().item() / (res_std ** 3)
        else:
            res_skew = 0.0
        
        # æœ€ç»ˆæ­¥æ®‹å·®
        res_last = true_last - pred_last
        res_last_mean = res_last.mean().item()
        
        diag["residual_mean"] = round(res_mean, 5)
        diag["residual_std"] = round(res_std, 5)
        diag["residual_abs_mean"] = round(res_abs_mean, 5)
        diag["residual_skew"] = round(res_skew, 3)
        diag["residual_last_step_mean"] = round(res_last_mean, 5)
        
        # --- 4. åŒºé—´å®½åº¦ ---
        pred_q10 = preds[..., 0]
        pred_q90 = preds[..., 4]
        interval_width = pred_q90 - pred_q10  # (B, Steps)
        
        diag["interval_width_mean"] = round(interval_width.mean().item(), 5)
        diag["interval_width_std"] = round(interval_width.std().item(), 5)
        diag["interval_width_min"] = round(interval_width.min().item(), 5)
        
        # è¦†ç›–ç‡: çœŸå€¼è½åœ¨ [Q10, Q90] å†…çš„æ¯”ä¾‹
        covered = ((true_vals >= pred_q10) & (true_vals <= pred_q90)).float()
        diag["coverage_q10_q90"] = round(covered.mean().item(), 4)
        
        # --- 5. é¢„æµ‹åŠ¨æ€èŒƒå›´ vs çœŸå€¼åŠ¨æ€èŒƒå›´ ---
        pred_range = pred_q50.max().item() - pred_q50.min().item()
        true_range = true_vals.max().item() - true_vals.min().item()
        diag["pred_dynamic_range"] = round(pred_range, 5)
        diag["true_dynamic_range"] = round(true_range, 5)
        diag["range_ratio"] = round(pred_range / max(true_range, 1e-8), 3)
        
        return diag

    @torch.no_grad()
    def _log_gradient_norms(self) -> dict:
        """è®¡ç®—å„å±‚æ¢¯åº¦èŒƒæ•°"""
        grad_norms = {}
        total_norm = 0.0
        for name, param in self.model.named_parameters():
            if param.grad is not None:
                norm = param.grad.data.norm(2).item()
                total_norm += norm ** 2
                # åªè®°å½•å…³é”®å±‚
                if any(k in name for k in ['input_proj', 'head', 'transformer', 'core', 'revin']):
                    grad_norms[name] = round(norm, 6)
        grad_norms["total"] = round(total_norm ** 0.5, 6)
        return grad_norms

    def _format_diagnostics(self, diag: dict, grad_norms: dict = None) -> str:
        """æ ¼å¼åŒ–è¯Šæ–­ä¿¡æ¯ä¸ºå¯è¯»æ—¥å¿—"""
        lines = []
        ep = diag["epoch"]
        sp = diag["split"]
        
        lines.append(f"  ğŸ”¬ [è¯Šæ–­ E{ep:03d}/{sp}]")
        lines.append(f"     è¾“å…¥: mean={diag['x_mean']}, std={diag['x_std']}, "
                     f"target_last: mean={diag['x_target_last_mean']}, std={diag['x_target_last_std']}")
        lines.append(f"     é¢„æµ‹Q50: mean={diag['pred_q50_mean']}, std={diag['pred_q50_std']} | "
                     f"çœŸå€¼: mean={diag['true_mean']}, std={diag['true_std']}")
        lines.append(f"     æ®‹å·®: mean={diag['residual_mean']}, std={diag['residual_std']}, "
                     f"skew={diag['residual_skew']}, |mean|={diag['residual_abs_mean']}")
        lines.append(f"     æœ€ç»ˆæ­¥åå·®: pred={diag['pred_last_mean']}, true={diag['true_last_mean']}, "
                     f"gap={diag['residual_last_step_mean']}")
        lines.append(f"     åŒºé—´[Q10,Q90]: width={diag['interval_width_mean']}Â±{diag['interval_width_std']}, "
                     f"min={diag['interval_width_min']}, coverage={diag['coverage_q10_q90']}")
        lines.append(f"     åŠ¨æ€èŒƒå›´: pred={diag['pred_dynamic_range']}, true={diag['true_dynamic_range']}, "
                     f"ratio={diag['range_ratio']}")
        
        if grad_norms:
            top_grads = sorted(grad_norms.items(), key=lambda x: x[1], reverse=True)[:5]
            grad_str = ", ".join(f"{k}={v}" for k, v in top_grads)
            lines.append(f"     æ¢¯åº¦: {grad_str}")
        
        return "\n".join(lines)

    def train(self, data_dict: Dict, verbose=False) -> float:
        # æ•°æ®ä¿ç•™åœ¨ CPUï¼Œç”¨ DataLoader æµæ°´çº¿é¢„åŠ è½½
        X_tr = torch.as_tensor(data_dict["X_tr"], dtype=torch.float32)
        y_tr = torch.as_tensor(data_dict["y_tr"], dtype=torch.float32)
        X_val = torch.as_tensor(data_dict["X_val"], dtype=torch.float32)
        y_val = torch.as_tensor(data_dict["y_val"], dtype=torch.float32)
        
        # [è¯Šæ–­] è®­ç»ƒå¼€å§‹å‰è¾“å‡ºæ•°æ®é›†ç»Ÿè®¡
        logging.info(f"  ğŸ“Š [æ•°æ®ç»Ÿè®¡] Train: X={list(X_tr.shape)}, y={list(y_tr.shape)} | "
                     f"Val: X={list(X_val.shape)}, y={list(y_val.shape)}")
        logging.info(f"     Train y: mean={y_tr.mean().item():.5f}, std={y_tr.std().item():.5f}, "
                     f"min={y_tr.min().item():.5f}, max={y_tr.max().item():.5f}")
        logging.info(f"     Val   y: mean={y_val.mean().item():.5f}, std={y_val.std().item():.5f}, "
                     f"min={y_val.min().item():.5f}, max={y_val.max().item():.5f}")
        
        # æ£€æµ‹ train/val åˆ†å¸ƒåç§»
        y_shift = abs(y_tr.mean().item() - y_val.mean().item())
        if y_shift > 0.5 * y_tr.std().item():
            logging.warning(f"  âš ï¸ [åˆ†å¸ƒåç§»] Train/Val yå‡å€¼å·®={y_shift:.4f} > 0.5Ã—std={0.5*y_tr.std().item():.4f}")
        
        # DataLoader: pin_memory + å¤š worker å®ç° CPU/GPU æµæ°´çº¿
        num_workers = int(getattr(self.cfg, "num_workers", 4))
        use_pin = (self.device.type == "cuda")
        train_dataset = TensorDataset(X_tr, y_tr)
        train_loader = DataLoader(
            train_dataset,
            batch_size=self.cfg.batch_size,
            shuffle=True,
            num_workers=num_workers,
            pin_memory=use_pin,
            persistent_workers=(num_workers > 0),
            drop_last=False
        )
        
        # éªŒè¯é›†ä¸€æ¬¡æ€§æ¬åˆ° GPUï¼ˆæ•°æ®é‡å°ï¼‰
        X_val_gpu = X_val.to(self.device)
        y_val_gpu = y_val.to(self.device)
        
        best_val_loss = float('inf')
        patience_counter = 0
        total_batches = len(train_loader)
        # [ä¿®å¤] å­¦ä¹ ç‡ warmup
        warmup_epochs = int(getattr(self.cfg, "warmup_epochs", 0))
        min_lr = float(getattr(self.cfg, "min_lr", 0.0))
        base_lr = float(getattr(self.cfg, "lr", 1e-3))
        
        for epoch in range(self.cfg.epochs):
            self.model.train()
            epoch_loss = 0.0
            batch_count = 0
            grad_norms = None

            # [ä¿®å¤] warmup è°ƒåº¦ï¼šå‰ warmup_epochs çº¿æ€§å‡æ¸©åˆ° base_lr
            if warmup_epochs > 0 and epoch < warmup_epochs:
                warmup_lr = min_lr + (base_lr - min_lr) * (epoch + 1) / warmup_epochs
                for pg in self.optimizer.param_groups:
                    pg["lr"] = warmup_lr
            
            for batch_idx, (X_batch, y_batch) in enumerate(train_loader):
                # æ¯ä¸ª batch æ¬åˆ° GPUï¼ˆDataLoader å·²é¢„åŠ è½½åˆ° pinned memoryï¼‰
                X_batch = X_batch.to(self.device, non_blocking=True)
                y_batch = y_batch.to(self.device, non_blocking=True)
                
                self.optimizer.zero_grad()
                with torch.cuda.amp.autocast(enabled=(self.device.type == "cuda")):
                    preds = self.model(X_batch)
                    loss = self.criterion(preds, y_batch)
                self.scaler.scale(loss).backward()
                
                # [è¯Šæ–­] åœ¨è¯Šæ–­ epoch çš„æœ€åä¸€ä¸ª batch è®°å½•æ¢¯åº¦
                if self._should_diagnose(epoch) and batch_idx == total_batches - 1:
                    self.scaler.unscale_(self.optimizer)
                    grad_norms = self._log_gradient_norms()
                
                self.scaler.step(self.optimizer)
                self.scaler.update()
                epoch_loss += loss.item()
                batch_count += 1
            
            avg_train_loss = epoch_loss / max(1, batch_count)
            self.history["train_loss"].append(avg_train_loss)
            avg_val_loss = self.validate(X_val_gpu, y_val_gpu)
            self.history["val_loss"].append(avg_val_loss)
            
            if verbose:
                logging.info(f"  ğŸ“ Epoch {epoch+1:03d} | Train: {avg_train_loss:.6f} | Val: {avg_val_loss:.6f}")
            
            # è¯Šæ–­è¾“å‡ºå·²å…³é—­ä»¥åŠ é€Ÿè®­ç»ƒ
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                self.best_state_dict = {k: v.cpu().clone() for k, v in self.model.state_dict().items()}
                patience_counter = 0
            else:
                patience_counter += 1
                if patience_counter >= self.cfg.patience: break
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        if hasattr(self, 'best_state_dict'):
            self.model.load_state_dict(self.best_state_dict)
        
        return best_val_loss

    def validate(self, X_val, y_val, batch_size=256):
        """åˆ†æ‰¹éªŒè¯ï¼Œé¿å…æ˜¾å­˜æº¢å‡º"""
        self.model.eval()
        total_loss = 0.0
        n_samples = len(X_val)
        n_batches = (n_samples + batch_size - 1) // batch_size

        with torch.no_grad():
            for i in range(n_batches):
                start = i * batch_size
                end = min((i + 1) * batch_size, n_samples)
                X_batch = X_val[start:end]
                y_batch = y_val[start:end]

                with torch.cuda.amp.autocast(enabled=(self.device.type == 'cuda')):
                    preds = self.model(X_batch)
                    loss = self.criterion(preds, y_batch)
                total_loss += loss.item() * (end - start)

        return total_loss / n_samples

    def predict(self, X, y_true_scaled=None, mc_samples=1):
        """
        é¢„æµ‹å‡½æ•°ï¼šæ”¯æŒ MC Dropout å¤šæ¬¡é‡‡æ ·å¹³å‡

        Args:
            X: è¾“å…¥æ•°æ®
            y_true_scaled: çœŸå€¼ï¼ˆå¯é€‰ï¼Œç”¨äºè®¡ç®—æ®‹å·®ï¼‰
            mc_samples: MCé‡‡æ ·æ¬¡æ•°ï¼Œ1è¡¨ç¤ºæ™®é€šæ¨ç†ï¼Œ>1è¡¨ç¤ºå¤šæ¬¡é‡‡æ ·å¹³å‡

        Returns:
            preds: (B, Steps, Quantiles) é¢„æµ‹ç»“æœ
        """
        self.model.eval()
        X = X.to(self.device)

        with torch.no_grad():
            with torch.cuda.amp.autocast(enabled=(self.device.type == "cuda")):
                # ç›´æ¥è°ƒç”¨ forwardï¼Œmc_samples>1 æ—¶è‡ªåŠ¨è¿”å›å¹³å‡å€¼
                if hasattr(self.model, 'forward') and 'mc_samples' in self.model.forward.__code__.co_varnames:
                    show_progress = mc_samples > 1  # å¤šæ¬¡é‡‡æ ·æ—¶æ˜¾ç¤ºè¿›åº¦æ¡
                    preds = self.model(X, mc_samples=mc_samples, show_progress=show_progress)
                else:
                    preds = self.model(X)

        preds_np = preds.cpu().numpy()

        # ä¿å­˜æ®‹å·®ç”¨äºåç»­åˆ†æ
        if y_true_scaled is not None:
            y_t = torch.as_tensor(y_true_scaled, dtype=torch.float32, device=self.device)
            if y_t.ndim == 3:
                y_t = y_t.squeeze(-1)
            pred_q50 = preds[..., 2]  # Q50
            self.last_residuals = (y_t - pred_q50).cpu().numpy()

        return preds_np

    def get_model_state(self):
        return self.model.state_dict()
