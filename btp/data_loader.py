# Copyright (c) 2026
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#     http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# data_loader.py
import os
import re
from typing import Tuple

import numpy as np
import pandas as pd
import logging

from btp.utils import pretty_title, normalize_col_name, make_unique_cols
from scipy import stats




class DataLoader:
    @staticmethod
    def load_xlsx(
        path: str,
        prefer_time_col: str = "æ—¶é—´",
        preview_rows: int = 10,
    ) -> Tuple[pd.DataFrame, float]:
        """
        ğŸ”§ å¢å¼ºç‰ˆï¼šé²æ£’çš„é‡‡æ ·é—´éš”æ¨æ–­
        """
        pretty_title("Step 1  è¯»å– Excel æ•°æ®ï¼ˆå¢å¼ºç‰ˆï¼‰")

        logging.info(f"[INFO] ç›®æ ‡æ–‡ä»¶: {path}")
        if not os.path.isfile(path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {path}")

        df = pd.read_excel(path)
        logging.info(f"[INFO] åŸå§‹å½¢çŠ¶: {df.shape}")

        # è§„èŒƒåŒ–åˆ—å
        df.columns = [normalize_col_name(c) for c in df.columns]
        df.columns = make_unique_cols(df.columns)

        # æŸ¥æ‰¾æ—¶é—´åˆ—
        time_col = None
        if prefer_time_col in df.columns:
            time_col = prefer_time_col
        else:
            for c in df.columns:
                if "æ—¶é—´" in c:
                    time_col = c
                    break

        if time_col is None:
            raise ValueError("æ‰¾ä¸åˆ°æ—¶é—´åˆ—")

        logging.info(f"[INFO] æ—¶é—´åˆ—: {time_col}")

        # è§£ææ—¶é—´
        raw_sample = str(df[time_col].iloc[0])
        if re.match(r"^\d{2}:\d{2}:\d{2}$", raw_sample):
            df["æ—¶é—´"] = pd.to_timedelta(df[time_col].astype(str), errors="coerce")
            base_date = pd.to_datetime("2000-01-01")
            df["æ—¶é—´"] = base_date + df["æ—¶é—´"]
        else:
            df["æ—¶é—´"] = pd.to_datetime(df[time_col], errors="coerce")

        # ä¸¢å¼ƒæ—¶é—´NaN
        before = len(df)
        df = df.dropna(subset=["æ—¶é—´"]).copy()
        after = len(df)
        logging.info(f"[INFO] å»é™¤æ—¶é—´NaN: {before} -> {after}")

        if after < 2:
            raise ValueError("æœ‰æ•ˆæ—¶é—´è¡Œä¸è¶³2è¡Œ")

        # æ’åº
        df = df.sort_values("æ—¶é—´").reset_index(drop=True)

        # å»é‡ï¼ˆä¿ç•™æœ€åä¸€æ¡ï¼‰
        dup_count = df["æ—¶é—´"].duplicated().sum()
        if dup_count > 0:
            logging.warning(f"[WARN] æ£€æµ‹åˆ° {dup_count} ä¸ªé‡å¤æ—¶é—´æˆ³ï¼Œä¿ç•™æœ€åä¸€æ¡")
            df = df.drop_duplicates(subset=["æ—¶é—´"], keep="last").reset_index(drop=True)

        # ğŸ”§ å¢å¼ºçš„é‡‡æ ·é—´éš”æ¨æ–­
        dt = df["æ—¶é—´"].diff().dt.total_seconds()
        dt_valid = dt[dt > 0]
        
        if dt_valid.empty:
            logging.error("[ERROR] æ— æ³•æ¨æ–­é‡‡æ ·é—´éš”ï¼ˆæ‰€æœ‰æ—¶é—´å·®â‰¤0ï¼‰ï¼Œè¯·æ£€æŸ¥æ•°æ®æº")
            raise ValueError("æ•°æ®æ—¶é—´åºåˆ—å¼‚å¸¸")
        
        # ä½¿ç”¨ä¼—æ•°è€Œéä¸­ä½æ•°ï¼ˆæ›´é²æ£’ï¼‰
        mode_result = stats.mode(dt_valid.round(1), keepdims=True)  # å››èˆäº”å…¥åˆ°0.1ç§’
        sampling_sec = float(mode_result.mode[0])
        
        # å¦‚æœä¼—æ•°ä¸å¯é ï¼Œå›é€€åˆ°ä¸­ä½æ•°
        if sampling_sec <= 0 or sampling_sec > 3600:
            sampling_sec = float(np.median(dt_valid))
            logging.warning(f"[WARN] ä¼—æ•°æ¨æ–­å¤±è´¥ï¼Œä½¿ç”¨ä¸­ä½æ•°: {sampling_sec:.3f}ç§’")
        
        logging.info(f"[INFO] æ¨æ–­é‡‡æ ·é—´éš”: {sampling_sec:.3f}ç§’ (ä¼—æ•°æ³•)")
        
        # éªŒè¯æ¨æ–­ç»“æœ
        expected_count = (df["æ—¶é—´"].iloc[-1] - df["æ—¶é—´"].iloc[0]).total_seconds() / sampling_sec
        actual_count = len(df)
        completeness = actual_count / expected_count * 100
        logging.info(f"[INFO] æ•°æ®å®Œæ•´åº¦: {completeness:.1f}% ({actual_count}/{int(expected_count)})")
        
        if completeness < 50:
            logging.warning("[WARN] æ•°æ®å®Œæ•´åº¦<50%ï¼Œå¯èƒ½å­˜åœ¨å¤§é‡é—´éš™")

        # æ£€æµ‹å¤§é—´éš™
        large_gaps = dt_valid[dt_valid > sampling_sec * 10]
        if len(large_gaps) > 0:
            logging.warning(
                f"[WARN] æ£€æµ‹åˆ° {len(large_gaps)} ä¸ªå¼‚å¸¸é—´éš™ï¼ˆæœ€å¤§: {large_gaps.max():.1f}ç§’ï¼‰"
            )

        logging.info(f"[OK] æ•°æ®åŠ è½½å®Œæˆï¼Œå½¢çŠ¶: {df.shape}\n")
        return df, sampling_sec
