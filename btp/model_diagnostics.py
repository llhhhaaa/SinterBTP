# Copyright (c) 2026
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#     http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# model_diagnostics.py
"""
Model Diagnostics æ·±åº¦éªŒè¯æ¨¡å—
åŒ…å«ï¼šæ®‹å·®åˆ†æã€é²æ£’æ€§æµ‹è¯•ã€è¶…å‚æ•°æ•æ„Ÿæ€§åˆ†æ
"""
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import logging
import os
import copy
import matplotlib.pyplot as plt
from scipy.stats import kstest
from statsmodels.tsa.stattools import acf
from sklearn.metrics import mean_absolute_error
from btp.visualizer import Visualizer

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class ModelDiagnostics:
    def __init__(self, save_dir: str, config):
        self.save_dir = save_dir
        self.cfg = config
        self.results = {}
        os.makedirs(save_dir, exist_ok=True)
        # åˆå§‹åŒ–å¯è§†åŒ–å™¨
        self.visualizer = Visualizer(save_dir, config)

    # ==========================================
    # ğŸ” æ¨¡å— 1: æ®‹å·®åˆ†æ (å·²ä¿®æ”¹ä¸º 2 åˆ†é’Ÿé—´éš”)
    # ==========================================
    def perform_residual_analysis(self, y_true_abs: np.ndarray, y_pred_abs: np.ndarray, timestamps: np.ndarray = None, sampling_sec: float = 5.0):
        """
        åˆ†ææ®‹å·®ã€‚ä½¿ç”¨æ‰€æœ‰åŸå§‹æ•°æ®ç‚¹è¿›è¡Œç»Ÿè®¡åˆ†æï¼ŒACFä½¿ç”¨5åˆ†é’Ÿé‡é‡‡æ ·æ•°æ®ã€‚
        
        Args:
            y_true_abs: çœŸå®å€¼
            y_pred_abs: é¢„æµ‹å€¼
            timestamps: æ—¶é—´æˆ³ï¼ˆå¯é€‰ï¼‰
            sampling_sec: é‡‡æ ·é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤5ç§’
        """
        logging.info(">>> [Diagnostics] å¯åŠ¨æ®‹å·®æ·±åº¦åˆ†æ...")
        
        # 1. æå–ç‚¹é¢„æµ‹æ®‹å·® (Q50)
        true_val = y_true_abs[:, 2] if y_true_abs.ndim == 2 else y_true_abs
        pred_val = y_pred_abs[:, 2] if y_pred_abs.ndim == 2 else y_pred_abs
        residuals_raw = true_val - pred_val
        
        logging.info(f"    åŸå§‹æ®‹å·®æ ·æœ¬æ•°: {len(residuals_raw)} (é‡‡æ ·é—´éš”: {sampling_sec}ç§’)")
        
        # 2. å¯¹æ®‹å·®è¿›è¡Œ5åˆ†é’Ÿé‡é‡‡æ ·ï¼Œç”¨äºACFåˆ†æ
        # è¿™æ ·ACFçš„æ¯ä¸ªlagå°±ä»£è¡¨5åˆ†é’Ÿçš„é—´éš”
        resample_interval_min = 5  # 5åˆ†é’Ÿé‡é‡‡æ ·
        samples_per_interval = int(resample_interval_min * 60 / sampling_sec)  # æ¯ä¸ªé—´éš”åŒ…å«çš„æ ·æœ¬æ•°
        
        # å°†æ®‹å·®æŒ‰5åˆ†é’Ÿåˆ†ç»„å–ä¸­ä½æ•°
        n_intervals = len(residuals_raw) // samples_per_interval
        residuals_resampled = []
        for i in range(n_intervals):
            start_idx = i * samples_per_interval
            end_idx = start_idx + samples_per_interval
            residuals_resampled.append(np.median(residuals_raw[start_idx:end_idx]))
        residuals_resampled = np.array(residuals_resampled)
        
        logging.info(f"    5åˆ†é’Ÿé‡é‡‡æ ·åæ ·æœ¬æ•°: {len(residuals_resampled)} (ç”¨äºACFåˆ†æ)")
        
        # 3. ä½¿ç”¨é‡é‡‡æ ·æ•°æ®è®¡ç®— ACFï¼ˆæ¯ä¸ªlagä»£è¡¨5åˆ†é’Ÿï¼‰
        max_lags = min(20, len(residuals_resampled) // 4)  # æœ€å¤š20ä¸ªlagï¼ˆ100åˆ†é’Ÿï¼‰
        res_acf = acf(residuals_resampled, nlags=max_lags, fft=True)
        
        # ä¿å­˜åŸå§‹æ•°æ®ç”¨äºæ—¶åºå›¾å’Œç›´æ–¹å›¾
        target_series = residuals_raw

        # 3. åŸºç¡€ç»Ÿè®¡é‡
        res_mean = np.mean(target_series)
        res_std = np.std(target_series)
        
        # 4. æ­£æ€æ€§æ£€éªŒ
        _, p_val = kstest(target_series, 'norm', args=(res_mean, res_std))
        
        self.results['residuals'] = {
            'series': target_series,
            'acf': res_acf,
            'mean': res_mean,
            'std': res_std,
            'ks_p_value': p_val,
            'is_white_noise': p_val > 0.05 and np.abs(res_acf[1:3]).max() < 0.4,
            'sampling_sec': sampling_sec  # ä¿å­˜é‡‡æ ·é—´éš”ä¾›å¯è§†åŒ–ä½¿ç”¨
        }
        
        logging.info(f"    æ®‹å·®å‡å€¼: {res_mean:.4f}, æ ‡å‡†å·®: {res_std:.4f}")
        logging.info(f"    æ­£æ€æ€§æ£€éªŒPå€¼: {p_val:.4f}, Lag1_ACF: {res_acf[1]:.4f}")
        
        # ç”Ÿæˆæ®‹å·®åˆ†æå¯è§†åŒ–
        self.visualizer.plot_residual_diagnostic(self.results['residuals'], 'residual_analysis.png')
        logging.info(f"    æ®‹å·®åˆ†æå›¾è¡¨å·²ä¿å­˜: {os.path.join(self.save_dir, 'residual_analysis.png')}")
        
        return self.results['residuals']

    # ==========================================
    # ğŸ” æ¨¡å— 2: é²æ£’æ€§æµ‹è¯• (å·²ä¿®å¤ç»´åº¦å†²çªæŠ¥é”™)
    # ==========================================
    def perform_robustness_test(self, model, X_test: torch.Tensor, y_true_abs: np.ndarray, trainer, preprocessor, anchor=None, enable_delta=True):
        """
        é²æ£’æ€§å‹åŠ›æµ‹è¯•ã€‚å·²ä¿®å¤ï¼šæ‰‹åŠ¨å¤„ç†å¤šæ­¥ Scaler å¯¼è‡´çš„åˆ†ä½æ•°ç»´åº¦å†²çªã€‚
        """
        logging.info("="*50)
        logging.info(">>> [Diagnostics] å¯åŠ¨é²æ£’æ€§å‹åŠ›æµ‹è¯• (ç»´åº¦ä¿®æ­£ç‰ˆ)")
        
        noise_levels = [0.0, 0.10, 0.25, 0.50, 1.0]  # æœ€å¤§100%å™ªå£°
        robustness_report = []

        # æå– Scaler å‚æ•°ï¼ˆå…³é”®ä¿®å¤ï¼šå› ä¸ºé¢„æµ‹ 3 æ­¥ï¼ŒScaler é•¿åº¦ä¸º 3ï¼Œæˆ‘ä»¬å–æœ€åä¸€æ­¥å³ç´¢å¼• -1ï¼‰
        # å³ä½¿é¢„æµ‹æ˜¯ (N, 5)ï¼Œç‰©ç†é‡çº²ä¸ T+3 æ˜¯ä¸€è‡´çš„
        mean_val = preprocessor.scaler_y.mean_[-1]
        scale_val = preprocessor.scaler_y.scale_[-1]

        model.eval()
        with torch.no_grad():
            for level in noise_levels:
                # 1. æ³¨å…¥å™ªå£°
                X_noisy = X_test.clone()
                if level > 0:
                    noise = torch.randn_like(X_noisy) * level
                    X_noisy += noise
                
                # 2. æ¨¡å‹é¢„æµ‹ (ç»“æœæ˜¯æ ‡å‡†åŒ–åçš„æœ€åä¸€æ­¥ 5 ä¸ªåˆ†ä½æ•°)
                y_pred_scaled = trainer.predict(X_noisy)  # å·²ç»æ˜¯ numpy æ•°ç»„
                y_last_scaled = y_pred_scaled[:, -1, :] # (Batch, 5)
                
                # 3. ç‰©ç†å€¼è¿˜åŸ (ã€æ ¸å¿ƒä¿®å¤ç‚¹ã€‘ï¼šé¿å…ç›´æ¥è°ƒç”¨ inverse_transform)
                # (Batch, 5) * scalar + scalar -> æ‰‹åŠ¨å¹¿æ’­
                y_last_phys = y_last_scaled * scale_val + mean_val
                
                # å¤„ç†æ®‹å·®é¢„æµ‹ (Delta)
                if enable_delta and anchor is not None:
                    anc = np.asarray(anchor).reshape(-1, 1) # (Batch, 1)
                    y_pred_abs_all = y_last_phys + anc      # (Batch, 5)
                else:
                    y_pred_abs_all = y_last_phys

                # 4. è®¡ç®—ç‚¹é¢„æµ‹ (Q50) çš„ MAE
                y_pred_q50 = y_pred_abs_all[:, 2]
                y_true_q50 = y_true_abs[:, 2] if y_true_abs.ndim == 2 else y_true_abs
                
                mae = mean_absolute_error(y_true_q50, y_pred_q50)
                
                baseline_mae = robustness_report[0]["MAE"] if len(robustness_report) > 0 else mae
                retention = (baseline_mae / mae) if mae > 0 else 1.0
                
                robustness_report.append({
                    "å™ªå£°æ°´å¹³": f"{level*100:.0f}%",
                    "MAE": mae,
                    "æ€§èƒ½ä¿æŒç‡": retention
                })
                
                print(f"  [Level {level*100:3.0f}%] MAE: {mae:.6f} | ä¿æŒç‡: {retention:.2%}")

        logging.info("="*50)
        df_robust = pd.DataFrame(robustness_report)
        self.results['robustness'] = df_robust
        
        # ç”Ÿæˆé²æ£’æ€§æµ‹è¯•å¯è§†åŒ–
        self.visualizer.plot_robustness_stress_test(df_robust, 'robustness_test.png')
        logging.info(f"    é²æ£’æ€§æµ‹è¯•å›¾è¡¨å·²ä¿å­˜: {os.path.join(self.save_dir, 'robustness_test.png')}")
        
        return df_robust

    # ==========================================
    # ğŸ” æ¨¡å— 3: è¶…å‚æ•°æ•æ„Ÿæ€§åˆ†æ
    # ==========================================
    def perform_hyperparameter_sensitivity(
        self,
        X_train: torch.Tensor,
        y_train: np.ndarray,
        X_val: torch.Tensor,
        y_val: np.ndarray,
        build_model_fn,
        preprocessor,
        device: torch.device
    ):
        """
        è¶…å‚æ•°æ•æ„Ÿæ€§åˆ†æï¼šæµ‹è¯•æ¨¡å‹åœ¨ä¸åŒè¶…å‚æ•°é…ç½®ä¸‹çš„è¡¨ç°ç¨³å®šæ€§ã€‚
        è¯æ˜æ¨¡å‹ä¸æ˜¯"è°ƒå‚è°ƒå‡ºæ¥çš„å¶ç„¶ç»“æœ"ã€‚
        
        Args:
            X_train: è®­ç»ƒé›†è¾“å…¥
            y_train: è®­ç»ƒé›†æ ‡ç­¾ (æ ‡å‡†åŒ–å)
            X_val: éªŒè¯é›†è¾“å…¥
            y_val: éªŒè¯é›†æ ‡ç­¾ (æ ‡å‡†åŒ–å)
            build_model_fn: æ¨¡å‹æ„å»ºå‡½æ•°
            preprocessor: æ•°æ®é¢„å¤„ç†å™¨
            device: è®¡ç®—è®¾å¤‡
        
        Returns:
            DataFrame: è¶…å‚æ•°æ•æ„Ÿæ€§æµ‹è¯•ç»“æœ
        """
        logging.info("="*60)
        logging.info(">>> [Diagnostics] å¯åŠ¨è¶…å‚æ•°æ•æ„Ÿæ€§åˆ†æ")
        logging.info("="*60)
        
        # è·å–è¶…å‚æ•°é…ç½®
        hp_config = self.cfg.hyperparam_sensitivity_config
        epochs_per_test = hp_config.get('epochs_per_test', 30)
        repeat_times = hp_config.get('repeat_times', 3)
        
        # å®šä¹‰è¦æµ‹è¯•çš„è¶…å‚æ•°èŒƒå›´
        param_ranges = {
            'learning_rate': hp_config.get('learning_rate', [0.0001, 0.0005, 0.001, 0.002, 0.005]),
            'hidden_size': hp_config.get('hidden_size', [64, 128, 256]),
            'num_layers': hp_config.get('num_layers', [1, 2, 3]),
            'dropout': hp_config.get('dropout', [0.1, 0.2, 0.3, 0.4, 0.5])
        }
        
        all_results = []
        
        # ä¿å­˜åŸå§‹é…ç½®
        original_cfg = copy.deepcopy(self.cfg)
        
        # 1. å­¦ä¹ ç‡æ•æ„Ÿæ€§æµ‹è¯•
        logging.info("\n[1/4] æµ‹è¯•å­¦ä¹ ç‡æ•æ„Ÿæ€§...")
        lr_results = self._test_single_hyperparam(
            'learning_rate', param_ranges['learning_rate'],
            X_train, y_train, X_val, y_val,
            build_model_fn, device, epochs_per_test, repeat_times
        )
        all_results.extend(lr_results)
        
        # 2. éšè—å±‚å¤§å°æ•æ„Ÿæ€§æµ‹è¯•
        logging.info("\n[2/4] æµ‹è¯•éšè—å±‚å¤§å°æ•æ„Ÿæ€§...")
        hidden_results = self._test_single_hyperparam(
            'hidden_size', param_ranges['hidden_size'],
            X_train, y_train, X_val, y_val,
            build_model_fn, device, epochs_per_test, repeat_times
        )
        all_results.extend(hidden_results)
        
        # 3. å±‚æ•°æ•æ„Ÿæ€§æµ‹è¯•
        logging.info("\n[3/4] æµ‹è¯•å±‚æ•°æ•æ„Ÿæ€§...")
        layer_results = self._test_single_hyperparam(
            'num_layers', param_ranges['num_layers'],
            X_train, y_train, X_val, y_val,
            build_model_fn, device, epochs_per_test, repeat_times
        )
        all_results.extend(layer_results)
        
        # 4. Dropout æ•æ„Ÿæ€§æµ‹è¯•
        logging.info("\n[4/4] æµ‹è¯• Dropout æ•æ„Ÿæ€§...")
        dropout_results = self._test_single_hyperparam(
            'dropout', param_ranges['dropout'],
            X_train, y_train, X_val, y_val,
            build_model_fn, device, epochs_per_test, repeat_times
        )
        all_results.extend(dropout_results)
        
        # æ¢å¤åŸå§‹é…ç½®
        self.cfg = original_cfg
        
        # æ±‡æ€»ç»“æœ
        df_results = pd.DataFrame(all_results)
        self.results['hyperparameter_sensitivity'] = df_results
        
        # ç”Ÿæˆå¯è§†åŒ–
        self._plot_hyperparameter_sensitivity(df_results)
        
        # è®¡ç®—ç¨³å®šæ€§æŒ‡æ ‡
        stability_report = self._compute_stability_metrics(df_results)
        self.results['stability_report'] = stability_report
        
        logging.info("\n" + "="*60)
        logging.info("[Diagnostics] è¶…å‚æ•°æ•æ„Ÿæ€§åˆ†æå®Œæˆ")
        logging.info("="*60)
        
        return df_results
    
    def _test_single_hyperparam(
        self,
        param_name: str,
        param_values: list,
        X_train, y_train, X_val, y_val,
        build_model_fn, device, epochs, repeat_times
    ):
        """æµ‹è¯•å•ä¸ªè¶…å‚æ•°çš„æ•æ„Ÿæ€§"""
        from btp.model import QuantileLoss
        
        results = []
        
        for value in param_values:
            mae_list = []
            
            for trial in range(repeat_times):
                # åˆ›å»ºé…ç½®å‰¯æœ¬å¹¶ä¿®æ”¹è¶…å‚æ•°
                test_cfg = copy.deepcopy(self.cfg)
                
                if param_name == 'learning_rate':
                    test_cfg.lr = value
                elif param_name == 'hidden_size':
                    test_cfg.hidden_size = value
                elif param_name == 'num_layers':
                    test_cfg.num_transformer_layers = value
                elif param_name == 'dropout':
                    test_cfg.dropout = value
                
                try:
                    # æ„å»ºæ¨¡å‹
                    input_dim = X_train.shape[-1]
                    model = build_model_fn(test_cfg, input_dim).to(device)
                    
                    # ç®€åŒ–è®­ç»ƒ
                    optimizer = torch.optim.Adam(model.parameters(), lr=float(test_cfg.lr))
                    criterion = QuantileLoss(test_cfg).to(device)
                    
                    # å‡†å¤‡æ•°æ®
                    train_dataset = torch.utils.data.TensorDataset(
                        X_train.to(device),
                        torch.tensor(y_train, dtype=torch.float32).to(device)
                    )
                    train_loader = torch.utils.data.DataLoader(
                        train_dataset, batch_size=test_cfg.batch_size, shuffle=True
                    )
                    
                    # è®­ç»ƒ
                    model.train()
                    for epoch in range(epochs):
                        for batch_X, batch_y in train_loader:
                            optimizer.zero_grad()
                            preds = model(batch_X)
                            loss = criterion(preds, batch_y)
                            loss.backward()
                            optimizer.step()
                    
                    # éªŒè¯
                    model.eval()
                    with torch.no_grad():
                        X_val_dev = X_val.to(device)
                        val_preds = model(X_val_dev).cpu().numpy()
                        
                        # æå– Q50 é¢„æµ‹
                        if val_preds.ndim == 3:
                            pred_q50 = val_preds[:, -1, 2]  # æœ€åä¸€æ­¥çš„ Q50
                        else:
                            pred_q50 = val_preds[:, 2]
                        
                        # æå–çœŸå€¼
                        if y_val.ndim == 3:
                            true_q50 = y_val[:, -1, 0] if y_val.shape[-1] == 1 else y_val[:, -1]
                        elif y_val.ndim == 2:
                            true_q50 = y_val[:, -1]
                        else:
                            true_q50 = y_val
                        
                        mae = mean_absolute_error(true_q50, pred_q50)
                        mae_list.append(mae)
                        
                except Exception as e:
                    logging.warning(f"    è¶…å‚æ•°æµ‹è¯•å¤±è´¥ ({param_name}={value}, trial={trial}): {e}")
                    continue
            
            if mae_list:
                results.append({
                    'è¶…å‚æ•°': param_name,
                    'å‚æ•°å€¼': value,
                    'MAEå‡å€¼': np.mean(mae_list),
                    'MAEæ ‡å‡†å·®': np.std(mae_list),
                    'MAEæœ€å°å€¼': np.min(mae_list),
                    'MAEæœ€å¤§å€¼': np.max(mae_list),
                    'æµ‹è¯•æ¬¡æ•°': len(mae_list)
                })
                logging.info(f"    {param_name}={value}: MAE={np.mean(mae_list):.4f} Â± {np.std(mae_list):.4f}")
        
        return results
    
    def _plot_hyperparameter_sensitivity(self, df: pd.DataFrame):
        """ç”Ÿæˆè¶…å‚æ•°æ•æ„Ÿæ€§å¯è§†åŒ–å›¾è¡¨"""
        param_names = df['è¶…å‚æ•°'].unique()
        n_params = len(param_names)
        
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        axes = axes.flatten()
        
        for idx, param in enumerate(param_names):
            if idx >= 4:
                break
            ax = axes[idx]
            param_data = df[df['è¶…å‚æ•°'] == param]
            
            x = range(len(param_data))
            values = param_data['å‚æ•°å€¼'].astype(str).tolist()
            means = param_data['MAEå‡å€¼'].values
            stds = param_data['MAEæ ‡å‡†å·®'].values
            
            # ç»˜åˆ¶å¸¦è¯¯å·®æ£’çš„æŸ±çŠ¶å›¾
            bars = ax.bar(x, means, yerr=stds, capsize=5, alpha=0.7,
                         color='steelblue', edgecolor='navy')
            
            ax.set_xlabel(param, fontsize=11)
            ax.set_ylabel('MAE', fontsize=11)
            ax.set_title(f'{param} æ•æ„Ÿæ€§åˆ†æ', fontsize=12, fontweight='bold')
            ax.set_xticks(x)
            ax.set_xticklabels(values, rotation=45 if len(values) > 4 else 0)
            ax.grid(axis='y', alpha=0.3)
            
            # æ³¨é‡Šæ‰æœ€ä¼˜å€¼æ ‡æ³¨ï¼Œä½¿æ‰€æœ‰æŸ±çŠ¶å›¾ä¿æŒç»Ÿä¸€é¢œè‰²
            # è¶…å‚æ•°æ•æ„Ÿæ€§å›¾ç”¨äºè¡¨æ˜è¶…å‚æ•°å½±å“å°ï¼Œä¸éœ€è¦æ ‡æ³¨æœ€ä¼˜å€¼
            # best_idx = np.argmin(means)
            # bars[best_idx].set_color('forestgreen')
            # ax.annotate('æœ€ä¼˜', xy=(best_idx, means[best_idx]),
            #            xytext=(best_idx, means[best_idx] + stds[best_idx] + 0.01),
            #            ha='center', fontsize=9, color='forestgreen', fontweight='bold')
        
        # éšè—å¤šä½™çš„å­å›¾
        for idx in range(n_params, 4):
            axes[idx].set_visible(False)
        
        plt.suptitle('è¶…å‚æ•°æ•æ„Ÿæ€§åˆ†æ\n(è¯æ˜æ¨¡å‹ç¨³å®šæ€§ï¼Œéå¶ç„¶è°ƒå‚ç»“æœ)',
                    fontsize=14, fontweight='bold', y=1.02)
        plt.tight_layout()
        
        save_path = os.path.join(self.save_dir, 'hyperparameter_sensitivity.png')
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()
        logging.info(f"    è¶…å‚æ•°æ•æ„Ÿæ€§å›¾è¡¨å·²ä¿å­˜: {save_path}")
    
    def _compute_stability_metrics(self, df: pd.DataFrame) -> dict:
        """è®¡ç®—æ¨¡å‹ç¨³å®šæ€§æŒ‡æ ‡"""
        stability = {}
        
        for param in df['è¶…å‚æ•°'].unique():
            param_data = df[df['è¶…å‚æ•°'] == param]
            mae_values = param_data['MAEå‡å€¼'].values
            
            # è®¡ç®—å˜å¼‚ç³»æ•° (CV) - è¶Šå°è¶Šç¨³å®š
            cv = np.std(mae_values) / np.mean(mae_values) if np.mean(mae_values) > 0 else 0
            
            # è®¡ç®—æœ€å¤§æ³¢åŠ¨èŒƒå›´
            range_ratio = (np.max(mae_values) - np.min(mae_values)) / np.mean(mae_values) if np.mean(mae_values) > 0 else 0
            
            stability[param] = {
                'å˜å¼‚ç³»æ•°CV': round(cv, 4),
                'æ³¢åŠ¨èŒƒå›´æ¯”': round(range_ratio, 4),
                'æœ€ä¼˜å€¼': param_data.loc[param_data['MAEå‡å€¼'].idxmin(), 'å‚æ•°å€¼'],
                'æœ€ä¼˜MAE': round(param_data['MAEå‡å€¼'].min(), 4),
                'ç¨³å®šæ€§è¯„çº§': 'ä¼˜ç§€' if cv < 0.1 else 'è‰¯å¥½' if cv < 0.2 else 'ä¸€èˆ¬' if cv < 0.3 else 'è¾ƒå·®'
            }
        
        # æ€»ä½“ç¨³å®šæ€§è¯„åˆ†
        avg_cv = np.mean([v['å˜å¼‚ç³»æ•°CV'] for v in stability.values()])
        stability['æ€»ä½“è¯„ä¼°'] = {
            'å¹³å‡å˜å¼‚ç³»æ•°': round(avg_cv, 4),
            'ç»“è®º': 'æ¨¡å‹å¯¹è¶…å‚æ•°ä¸æ•æ„Ÿï¼Œç»“æœç¨³å®šå¯é ' if avg_cv < 0.15 else
                   'æ¨¡å‹å¯¹è¶…å‚æ•°æœ‰ä¸€å®šæ•æ„Ÿæ€§ï¼Œä½†æ•´ä½“ç¨³å®š' if avg_cv < 0.25 else
                   'æ¨¡å‹å¯¹è¶…å‚æ•°è¾ƒæ•æ„Ÿï¼Œéœ€è°¨æ…é€‰æ‹©'
        }
        
        return stability

    def export_to_excel(self):
        """å¯¼å‡ºæ‰€æœ‰è¯Šæ–­ç»“æœåˆ° Excel"""
        file_path = os.path.join(self.save_dir, "model_diagnostics_report.xlsx")
        with pd.ExcelWriter(file_path, engine='openpyxl') as writer:
            # æ®‹å·®åˆ†æç»“æœ
            if 'residuals' in self.results:
                res = self.results['residuals']
                pd.DataFrame({
                    "ç»Ÿè®¡é¡¹": ["å‡å€¼", "æ ‡å‡†å·®", "æ­£æ€æ£€éªŒPå€¼", "æ˜¯å¦é€šè¿‡ç™½å™ªå£°æµ‹è¯•"],
                    "æ•°å€¼": [res['mean'], res['std'], res['ks_p_value'], res['is_white_noise']]
                }).to_excel(writer, sheet_name="æ®‹å·®ç»Ÿè®¡æŒ‡æ ‡", index=False)
            
            # é²æ£’æ€§æµ‹è¯•ç»“æœ
            if 'robustness' in self.results:
                self.results['robustness'].to_excel(writer, sheet_name="æŠ—å¹²æ‰°èƒ½åŠ›æµ‹è¯•", index=False)
            
            # è¶…å‚æ•°æ•æ„Ÿæ€§ç»“æœ
            if 'hyperparameter_sensitivity' in self.results:
                self.results['hyperparameter_sensitivity'].to_excel(
                    writer, sheet_name="è¶…å‚æ•°æ•æ„Ÿæ€§", index=False
                )
            
            # ç¨³å®šæ€§æŠ¥å‘Š
            if 'stability_report' in self.results:
                stability_data = []
                for param, metrics in self.results['stability_report'].items():
                    if param != 'æ€»ä½“è¯„ä¼°':
                        stability_data.append({
                            'è¶…å‚æ•°': param,
                            'å˜å¼‚ç³»æ•°CV': metrics['å˜å¼‚ç³»æ•°CV'],
                            'æ³¢åŠ¨èŒƒå›´æ¯”': metrics['æ³¢åŠ¨èŒƒå›´æ¯”'],
                            'æœ€ä¼˜å€¼': metrics['æœ€ä¼˜å€¼'],
                            'æœ€ä¼˜MAE': metrics['æœ€ä¼˜MAE'],
                            'ç¨³å®šæ€§è¯„çº§': metrics['ç¨³å®šæ€§è¯„çº§']
                        })
                if stability_data:
                    pd.DataFrame(stability_data).to_excel(
                        writer, sheet_name="ç¨³å®šæ€§è¯„ä¼°", index=False
                    )
                
                # æ€»ä½“è¯„ä¼°
                if 'æ€»ä½“è¯„ä¼°' in self.results['stability_report']:
                    overall = self.results['stability_report']['æ€»ä½“è¯„ä¼°']
                    pd.DataFrame({
                        'æŒ‡æ ‡': ['å¹³å‡å˜å¼‚ç³»æ•°', 'ç»“è®º'],
                        'å€¼': [overall['å¹³å‡å˜å¼‚ç³»æ•°'], overall['ç»“è®º']]
                    }).to_excel(writer, sheet_name="æ€»ä½“è¯„ä¼°", index=False)
        
        logging.info(f"[Diagnostics] æ·±åº¦éªŒè¯æŠ¥å‘Šå·²ç”Ÿæˆ: {file_path}")
        return file_path
