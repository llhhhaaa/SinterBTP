# Copyright (c) 2026
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#     http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


# preprocessor.py
# (ç²¾ç®€ç‰ˆ - åŸå§‹é«˜é¢‘åºåˆ—ï¼Œå·²ç§»é™¤ç²’åŒ–åŠŸèƒ½)

import re
import os
import hashlib
import pickle
from typing import Dict, List, Tuple, Optional, Any
from concurrent.futures import ProcessPoolExecutor, as_completed
from functools import partial
from scipy.interpolate import CubicSpline  
import numpy as np
import pandas as pd
import joblib
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import logging
from tqdm import tqdm
from numba import jit, prange

from btp.utils import pretty_title, normalize_col_name

# =========================================================
# 1. Numba åŠ é€Ÿæ ¸å¿ƒè®¡ç®—å‡½æ•° (å·²ç§»é™¤ç²’åŒ–ç›¸å…³å‡½æ•°)
# =========================================================



@jit(nopython=True, cache=True)
def _check_time_continuity(timestamps_sec: np.ndarray, max_gap: float) -> bool:
    if timestamps_sec.size < 2:
        return False
    for i in range(1, timestamps_sec.size):
        if timestamps_sec[i] - timestamps_sec[i-1] > max_gap:
            return False
    return True


@jit(nopython=True, cache=True)
def _check_validity_relaxed_numba(data: np.ndarray, min_ratio: float) -> bool:
    valid_count = np.sum(np.isfinite(data))
    total = data.size
    return (valid_count / total) >= min_ratio

def _extract_spline_features(df: pd.DataFrame, side: str) -> pd.DataFrame:
    """
    ç‰©ç†å¼•æ“ï¼šä½¿ç”¨ä¸‰æ¬¡æ ·æ¡æ’å€¼ä»é£ç®±æ¸©åº¦åºåˆ—ä¸­æå–å½¢çŠ¶ç‰¹å¾
    """
    # 1. å®šä¹‰ç‰©ç†åæ ‡ (é£ç®±ç¼–å·ä½œä¸º X è½´)
    x_nodes = np.array([15, 17, 18, 19, 20, 21, 22, 23, 24])
    temp_cols = [normalize_col_name(f"{i}#é£ç®±æ¸©åº¦({side})") for i in x_nodes]
    
    # æå–æ•°æ®çŸ©é˜µ
    y_matrix = df[temp_cols].values
    
    # å‡†å¤‡å­˜å‚¨å®¹å™¨
    calc_pos = []
    calc_temp = []
    calc_slope = []
    calc_auc = []

    # éå†æ¯ä¸€è¡Œ
    for i in range(len(y_matrix)):
        y = y_matrix[i]
        mask = np.isfinite(y)
        
        # é²æ£’æ€§æ£€æŸ¥ï¼šå¦‚æœæœ‰æ•ˆç‚¹å¤ªå°‘ï¼Œè¿”å›ç©ºå€¼
        if np.sum(mask) < 4:
            calc_pos.append(np.nan); calc_temp.append(np.nan)
            calc_slope.append(np.nan); calc_auc.append(np.nan)
            continue
            
        try:
            # æ‰§è¡Œä¸‰æ¬¡æ ·æ¡æ’å€¼
            cs = CubicSpline(x_nodes[mask], y[mask])
            
            # åœ¨é«˜å¯†åº¦ç©ºé—´å¯»æ‰¾æœ€å¤§å€¼ (BTP)
            x_dense = np.linspace(15, 24, 5000)
            y_dense = cs(x_dense)
            peak_idx = np.argmax(y_dense)
            
            p_pos = x_dense[peak_idx]
            p_temp = y_dense[peak_idx]
            
            # è®¡ç®— BTP ä½ç½®çš„ä¸€é˜¶å¯¼æ•°ï¼ˆæ–œç‡ï¼‰å’Œ 15-24 åŒºé—´çš„ç§¯åˆ†
            p_slope_pre = (cs(p_pos) - cs(p_pos - 1)) / 1.0 
            p_auc = cs.integrate(15, 24)
            
            calc_pos.append(p_pos); calc_temp.append(p_temp)
            calc_slope.append(p_slope_pre); calc_auc.append(p_auc)
        except:
            calc_pos.append(np.nan); calc_temp.append(np.nan)
            calc_slope.append(np.nan); calc_auc.append(np.nan)

    return pd.DataFrame({
        f"{side}ä¾§_è®¡ç®—BTPä½ç½®": calc_pos,
        f"{side}ä¾§_è®¡ç®—BTPæ¸©åº¦": calc_temp,
        f"{side}ä¾§_BTPæ–œç‡": calc_slope,
        f"{side}ä¾§_BTPç§¯åˆ†é¢ç§¯": calc_auc
    }, index=df.index)

# =========================================================
# 2. æ‰¹é‡åºåˆ—æ„å»ºå‡½æ•° (å•ä¸€å°ºåº¦)
# =========================================================
def _build_sequences_batch(
    batch_indices: np.ndarray,
    X_combined_all: np.ndarray,
    timestamps_all: np.ndarray,
    y_combined_all: np.ndarray,
    scale_param: Dict,
    tgt_w: int,
    K: int,
    granulated_feat_dim: int,  # ä¿ç•™å‚æ•°ä»¥å…¼å®¹è°ƒç”¨ç­¾åï¼Œä½†ä¸å†ä½¿ç”¨
    max_gap_sec: float,
    enable_delta: bool,
    sampling_sec: float = 5.0,
    forecast_steps: int = 5,
    prediction_offset: int = 0,
    y_raw_all: np.ndarray = None,
    max_future_index: Optional[int] = None
) -> Tuple:
    """
    æ„å»ºè®­ç»ƒåºåˆ—ï¼ˆå·²ç§»é™¤ç²’åŒ–åŠŸèƒ½ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹åºåˆ—ï¼‰
    
    è¿”å›:
        X_macro_batch: å ä½æ•°ç»„ (B, 1, 1)ï¼Œä¿æŒæ¥å£å…¼å®¹
        X_raw_batch: åŸå§‹åºåˆ— (B, raw_seq_len, feat_dim)
        y_batch: é¢„æµ‹ç›®æ ‡ (B, forecast_steps)
        anchor_batch: é”šç‚¹å€¼ (B, 1)
        y_obs_batch: è§‚æµ‹çª— (B, tgt_w)
        valid_count: æœ‰æ•ˆæ ·æœ¬æ•°
    """
    max_samples = len(batch_indices)
    rows_per_step = int(round(scale_param["w_rows"]))

    raw_seq_len = scale_param["buf_needed"]
    raw_feat_dim = X_combined_all.shape[1]

    # X_macro_batch ä½œä¸ºå ä½æ•°ç»„ï¼Œä¿æŒè¿”å›æ ¼å¼å…¼å®¹
    X_macro_batch = np.zeros((max_samples, 1, 1), dtype=np.float32)
    X_raw_batch = np.zeros((max_samples, raw_seq_len, raw_feat_dim), dtype=np.float32)
    
    y_batch = np.zeros((max_samples, forecast_steps), dtype=np.float32)
    anchor_batch = np.zeros((max_samples, 1), dtype=np.float32)
    y_obs_batch = np.full((max_samples, tgt_w), np.nan, dtype=np.float32)
    
    valid_count = 0
    timestamps_sec = timestamps_all.astype('datetime64[s]').astype(np.float64)
    total_len = len(y_combined_all)
    # [é˜²æ­¢æ ‡ç­¾æ³„éœ²] è®­ç»ƒ/éªŒè¯/æµ‹è¯•æ•°æ®æ„å»ºæ—¶é™åˆ¶æœªæ¥ç´¢å¼•ä¸Šç•Œ
    # max_future_index ä¸ºå…è®¸çš„æœ€å¤§ç´¢å¼•ï¼ˆå«ï¼‰ï¼Œé»˜è®¤ä¸é™åˆ¶
    future_index_cap = max_future_index if max_future_index is not None else total_len - 1

    for i in batch_indices:
        input_anchor = i - tgt_w
        # æ·»åŠ  prediction_offsetï¼šä»è¾“å…¥åºåˆ—æœ«å°¾è·³è¿‡ prediction_offset è¡Œåå¼€å§‹é¢„æµ‹
        future_indices = [i + prediction_offset + (s+1) * rows_per_step for s in range(forecast_steps)]
        
        # 1) ä¸èƒ½è¶…è¿‡æ•°æ®é•¿åº¦
        if future_indices[-1] >= total_len:
            continue
        # 2) ä¸èƒ½è¶…è¿‡åˆ†æ®µä¸Šç•Œï¼ˆç”¨äºé¿å…è®­ç»ƒæ ‡ç­¾æ³„éœ²åˆ°éªŒè¯/æµ‹è¯•ï¼‰
        if future_indices[-1] > future_index_cap:
            continue
        if i < raw_seq_len: continue
            
        future_vals = y_combined_all[future_indices]
        if np.any(np.isnan(future_vals)): continue
            
        # æŠ½å–åŸå§‹åºåˆ— (Raw Sequence)
        raw_slice = X_combined_all[i - raw_seq_len : i, :]
        raw_ts = timestamps_sec[i - raw_seq_len : i]
        if not _check_time_continuity(raw_ts, max_gap_sec): continue
        if not _check_validity_relaxed_numba(raw_slice, 0.8): continue
        
        X_raw_batch[valid_count] = raw_slice.astype(np.float32)
        
        # æ ‡ç­¾ä¸é”šç‚¹
        anchor_scalar = np.nanmean(y_combined_all[max(0, i-1):i+1]) if enable_delta else 0.0
        y_batch[valid_count] = future_vals - anchor_scalar if enable_delta else future_vals
        anchor_batch[valid_count] = anchor_scalar
        
        # è§‚æµ‹çª—ä½¿ç”¨åŸå§‹æœªå¹³æ»‘çš„ yï¼ˆç”¨äºå¯è§†åŒ–çœŸå®æ³¢åŠ¨èŒƒå›´ï¼‰
        obs_source = y_raw_all if y_raw_all is not None else y_combined_all
        y_obs_batch[valid_count] = obs_source[input_anchor : i].astype(np.float32)
        valid_count += 1
        
    return (
        X_macro_batch[:valid_count],
        X_raw_batch[:valid_count],
        y_batch[:valid_count],
        anchor_batch[:valid_count],
        y_obs_batch[:valid_count],
        valid_count
    )

# =========================================================
# 3. DataPreprocessor ä¸»ç±»
# =========================================================

class DataPreprocessor:
    """ğŸš€ æè‡´ç²¾ç®€ç‰ˆé¢„å¤„ç†å™¨ (Single Scale Only)"""

    def __init__(self, config):
        self.cfg = config
        self.scaler_core = StandardScaler()
        self.scaler_aux = StandardScaler()
        self.pca_aux = PCA(n_components=0.95, random_state=self.cfg.seed)
        self.scaler_y = StandardScaler()

        self.target_col = "BTP_pos_target"
        self.core_cols: List[str] = []
        self.aux_cols: List[str] = []
        self.input_cols: List[str] = []
        self.raw_input_cols: List[str] = []
        self.clip_limits: Dict[str, Tuple[float, float]] = {}

    def get_cache_path(self) -> str:
        """æ ¹æ®å½“å‰é…ç½®ç”Ÿæˆå“ˆå¸Œå€¼ä½œä¸ºç¼“å­˜æ–‡ä»¶å"""
        # æŒ‘é€‰å½±å“æ•°æ®é¢„å¤„ç†å’Œåˆ‡åˆ†çš„å…³é”®é…ç½®
        cache_params = {
            "raw_seq_len": self.cfg.raw_seq_len,
            "forecast_steps": self.cfg.forecast_steps,
            "prediction_offset": self.cfg.prediction_offset,
            "target_column": self.cfg.target_column,
            "cv_n_splits": self.cfg.cv_n_splits,
            "val_split": self.cfg.val_split,
            "test_split": self.cfg.test_split,
            "seed": self.cfg.seed,
            "target_smooth_span": getattr(self.cfg, "target_smooth_span", 0),
            "enable_delta_forecast": self.cfg.enable_delta_forecast,
            "optimize_gap_size": self.cfg.optimize_gap_size
        }
        param_str = str(sorted(cache_params.items()))
        hash_val = hashlib.md5(param_str.encode()).hexdigest()
        
        cache_dir = getattr(self.cfg, "CACHE_DIR", "data/cache")
        os.makedirs(cache_dir, exist_ok=True)
        return os.path.join(cache_dir, f"pregen_data_{hash_val}.pkl")

    def build_features(self, df: pd.DataFrame) -> pd.DataFrame:
        pretty_title("Step 2  ç‰¹å¾å·¥ç¨‹ (ç‰©ç†ç‰¹å¾å¼•æ“ç‰ˆ)")

        # è®°å½•åŸå§‹ Excel è¾“å…¥åˆ—ï¼ˆä»…ç”¨äºå¼‚å¸¸å€¼æˆªæ–­ï¼‰
        original_cols = list(df.columns)
        
        # 1. è‡ªåŠ¨æå–å—åŒ—ä¸¤ä¾§ç‰©ç†ç‰¹å¾
        logging.info("[Physics] æ­£åœ¨æ‰§è¡Œä¸‰æ¬¡æ ·æ¡æ’å€¼æå–ç‰©ç†æè¿°ç¬¦...")
        df_south = _extract_spline_features(df, side='å—')
        df_north = _extract_spline_features(df, side='åŒ—')
        df = pd.concat([df, df_south, df_north], axis=1)
        
        # 2. è®¾ç½®ç›®æ ‡å€¼
        target_source_col = self.cfg.target_column
        df[self.target_col] = pd.to_numeric(df[target_source_col], errors="coerce")

        # ä»…å¯¹åŸå§‹ Excel ä¼ æ„Ÿå™¨åˆ—åšæˆªæ–­ï¼ˆæ’é™¤æ—¶é—´åˆ—ä¸ç›®æ ‡åˆ—ï¼‰
        self.raw_input_cols = [
            c for c in original_cols if c not in ["æ—¶é—´", target_source_col]
        ]
        
        # 3. ç‰©ç†å¯¼æ•°ç‰¹å¾
        dt = df["æ—¶é—´"].diff().dt.total_seconds()
        prefix = "å—" if "å—" in target_source_col else "åŒ—"
        df["BTP_vel"] = df[f"{prefix}ä¾§_è®¡ç®—BTPä½ç½®"].diff() / dt
        df["BTP_acc"] = df["BTP_vel"].diff() / dt

        # 4. å®šä¹‰ Core Columns (Enhanced Transformer ä½¿ç”¨ï¼ŒåŒ…å«è¡ç”Ÿç‰¹å¾)
        self.core_cols = [
            "å—ä¾§_è®¡ç®—BTPä½ç½®", "å—ä¾§_è®¡ç®—BTPæ¸©åº¦", "å—ä¾§_BTPæ–œç‡", "å—ä¾§_BTPç§¯åˆ†é¢ç§¯",
            "åŒ—ä¾§_è®¡ç®—BTPä½ç½®", "åŒ—ä¾§_è®¡ç®—BTPæ¸©åº¦", "åŒ—ä¾§_BTPæ–œç‡", "åŒ—ä¾§_BTPç§¯åˆ†é¢ç§¯",
            "æœºé€Ÿæ£€æµ‹å€¼", "æ–™å±‚åšåº¦å¹³å‡å€¼", "å—ä¾§é£ç®±è´Ÿå‹", "BTP_vel", "BTP_acc"
        ]
        
        # 4.1 å®šä¹‰åŸºçº¿æ¨¡å‹ä¸“ç”¨çš„ Core Columns (ä¸å«è¡ç”Ÿç‰¹å¾ï¼Œåªç”¨åŸå§‹é£ç®±æ¸©åº¦)
        # è¡ç”Ÿç‰¹å¾åˆ—è¡¨ (ä¸‰æ¬¡æ ·æ¡æ’å€¼è®¡ç®—å¾—å‡ºï¼ŒåŸºçº¿æ¨¡å‹ç¦ç”¨)
        self.spline_derived_cols = [
            "å—ä¾§_è®¡ç®—BTPä½ç½®", "å—ä¾§_è®¡ç®—BTPæ¸©åº¦", "å—ä¾§_BTPæ–œç‡", "å—ä¾§_BTPç§¯åˆ†é¢ç§¯",
            "åŒ—ä¾§_è®¡ç®—BTPä½ç½®", "åŒ—ä¾§_è®¡ç®—BTPæ¸©åº¦", "åŒ—ä¾§_BTPæ–œç‡", "åŒ—ä¾§_BTPç§¯åˆ†é¢ç§¯",
            "BTP_vel", "BTP_acc"  # è¿™äº›ä¹Ÿä¾èµ–äºè®¡ç®—BTPä½ç½®
        ]
        
        # åŸºçº¿æ¨¡å‹ä½¿ç”¨çš„åŸå§‹é£ç®±æ¸©åº¦åˆ— (15#-24#)
        from btp.utils import normalize_col_name
        self.baseline_temp_cols = []
        for side in ['å—', 'åŒ—']:
            for box_id in [15, 17, 18, 19, 20, 21, 22, 23, 24]:
                col_name = normalize_col_name(f"{box_id}#é£ç®±æ¸©åº¦({side})")
                if col_name in df.columns:
                    self.baseline_temp_cols.append(col_name)
        
        # åŸºçº¿æ¨¡å‹æ ¸å¿ƒç‰¹å¾ = åŸå§‹é£ç®±æ¸©åº¦ + BTPä½ç½®(ä¿ç•™ç”¨äºç‰©ç†è·¯å¾„) + éè¡ç”Ÿçš„å·¥è‰ºå‚æ•°
        # æ³¨æ„ï¼šBTPä½ç½®æ˜¯ä¸‰æ¬¡æ ·æ¡è®¡ç®—çš„ï¼Œä½†ä½œä¸ºç›®æ ‡ç›¸å…³ç‰¹å¾éœ€è¦ä¿ç•™
        self.baseline_core_cols = (
            self.baseline_temp_cols +
            ["å—ä¾§_è®¡ç®—BTPä½ç½®", "åŒ—ä¾§_è®¡ç®—BTPä½ç½®"] +  # ä¿ç•™BTPä½ç½®ç”¨äºç‰©ç†è·¯å¾„
            ["æœºé€Ÿæ£€æµ‹å€¼", "æ–™å±‚åšåº¦å¹³å‡å€¼", "å—ä¾§é£ç®±è´Ÿå‹"]
        )
        
        # 5. å®šä¹‰ Aux Columns
        ignore_keywords = ["é£ç®±æ¸©åº¦", "è®¾å®šå€¼", "ä¸Šé™", "ä¸‹é™", "æŠ¥è­¦", "ä½ç½®", "æ¸©åº¦"]
        exclude_set = set(self.core_cols + [self.target_col, "æ—¶é—´"])
        
        self.aux_cols = []
        for c in df.columns:
            if c in exclude_set: continue
            if any(k in c for k in ignore_keywords): continue
            if pd.api.types.is_numeric_dtype(df[c]):
                self.aux_cols.append(c)
        
        self.input_cols = self.core_cols + self.aux_cols
        
        # åŸºçº¿æ¨¡å‹è¾“å…¥åˆ— (ä¸å«è¡ç”Ÿç‰¹å¾)
        self.baseline_input_cols = self.baseline_core_cols + self.aux_cols
        
        # [æ¶ˆè] å¦‚æœå…³é—­æ‹Ÿåˆæ¨¡å—ï¼Œé€€åŒ–ä¸ºåŸºçº¿ç‰¹å¾é›†ï¼ˆä¸å«ä¸‰æ¬¡æ ·æ¡è¡ç”Ÿç‰¹å¾ï¼‰
        if not getattr(self.cfg, "enable_fitting_module", True):
            logging.info("[Ablation] enable_fitting_module=False â†’ ä½¿ç”¨åŸºçº¿ç‰¹å¾é›† (ä¸å«ä¸‰æ¬¡æ ·æ¡è¡ç”Ÿç‰¹å¾)")
            self.core_cols = self.baseline_core_cols
            self.input_cols = self.baseline_input_cols
        
        for c in self.input_cols + [self.target_col]:
            df[c] = pd.to_numeric(df[c], errors="coerce")
            
        logging.info(f"[Features] ç‰©ç†æ ¸å¿ƒç‰¹å¾: {len(self.core_cols)}, è¾…åŠ©ç‰¹å¾: {len(self.aux_cols)}")
        return df

    def _fill_missing_values(self, df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:
        df = df.copy()
        if len(cols) == 0:
            return df
        df[cols] = df[cols].ffill().bfill()
        col_means = df[cols].mean()
        df[cols] = df[cols].fillna(col_means)
        return df

    def _compute_clip_limits(self, df: pd.DataFrame, cols: List[str]) -> None:
        for c in cols:
            q_low, q_high = df[c].quantile(0.01), df[c].quantile(0.99)
            if np.isfinite(q_low) and np.isfinite(q_high) and q_high > q_low:
                self.clip_limits[c] = (float(q_low), float(q_high))

    def _apply_clip_limits(self, df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:
        df = df.copy()
        for c in cols:
            if c in self.clip_limits:
                lower, upper = self.clip_limits[c]
                df[c] = df[c].clip(lower=lower, upper=upper)
        return df

    def _get_raw_clip_cols(self, df: pd.DataFrame) -> List[str]:
        return [c for c in self.raw_input_cols if c in df.columns and pd.api.types.is_numeric_dtype(df[c])]

    def _final_safety_net(self, df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:
        if len(cols) == 0:
            return df
        if df[cols].isnull().any().any():
            df[cols] = df[cols].fillna(method='ffill').fillna(method='bfill').fillna(0)
        return df


    def _smart_fill_small_gaps(self, df: pd.DataFrame, cols: List[str], time_col: str, max_gap_sec: float) -> pd.DataFrame:
        df = df.copy()
        time_diffs = df[time_col].diff().dt.total_seconds()
        for c in cols:
            mask_null = df[c].isnull()
            if not mask_null.any(): continue
            can_fill = mask_null & (time_diffs <= max_gap_sec)
            if can_fill.any():
                df.loc[can_fill, c] = df[c].fillna(method='ffill')[can_fill]
        return df

    def _internal_parallel_build(self, start_idx, end_idx, X_all, y_all, time_all, scale_param, tgt_w, K, granulated_feat_dim, max_gap_sec, sampling_sec, set_name, y_raw_all=None, max_future_index=None):
        num_candidates = end_idx - start_idx
        n_workers = min(os.cpu_count() or 4, 8)
        batch_size = max(100, num_candidates // (n_workers * 4))
        all_indices = np.arange(start_idx, end_idx)
        batches = [all_indices[i:i+batch_size] for i in range(0, len(all_indices), batch_size)]
        
        executor = ProcessPoolExecutor(max_workers=n_workers)
        prediction_offset = getattr(self.cfg, 'prediction_offset', 0)
        worker_func = partial(
            _build_sequences_batch, X_combined_all=X_all, timestamps_all=time_all, y_combined_all=y_all,
            scale_param=scale_param, tgt_w=tgt_w, K=K, granulated_feat_dim=granulated_feat_dim,
            max_gap_sec=max_gap_sec, enable_delta=self.cfg.enable_delta_forecast,
            forecast_steps=self.cfg.forecast_steps, prediction_offset=prediction_offset,
            sampling_sec=sampling_sec, y_raw_all=y_raw_all, max_future_index=max_future_index
        )
        
        futures = [executor.submit(worker_func, batch) for batch in batches]
        X_mac_list, X_raw_list, y_list, anchor_list, yobs_list, total_valid = [], [], [], [], [], 0

        pbar = tqdm(as_completed(futures), total=len(futures), desc=f"[{set_name}] æ„å»ºåºåˆ—", ncols=100)
        for future in pbar:
            xb_mac, xb_raw, yb, anc_b, yobs_b, vc = future.result()
            if vc > 0:
                X_mac_list.append(xb_mac); X_raw_list.append(xb_raw)
                y_list.append(yb); anchor_list.append(anc_b); yobs_list.append(yobs_b)
                total_valid += vc
        
        executor.shutdown()
        if total_valid == 0: raise ValueError(f"{set_name}æ— æœ‰æ•ˆæ ·æœ¬")
        
        return np.vstack(X_mac_list), np.vstack(X_raw_list), np.vstack(y_list), np.vstack(anchor_list), np.vstack(yobs_list)

    def yield_rolling_folds(self, df: pd.DataFrame, sampling_sec: float):
        """
        æ—¶åºç‰ˆäº¤å‰éªŒè¯ï¼Œæ¯æŠ˜éƒ½æœ‰ç‹¬ç«‹çš„ test é›†
        
        æ—¶é—´è½´: [0 ========================== 100%]
        
        Fold 1: [Train 50%    ][Val 10%][Test 10%]
        Fold 2: [Train 60%        ][Val 10%][Test 10%]
        Fold 3: [Train 70%            ][Val 10%][Test 10%]
        ...
        
        Args:
            df: æŒ‰æ—¶é—´æ’åºçš„æ•°æ®
            sampling_sec: é‡‡æ ·é—´éš”ï¼ˆç§’ï¼‰
            
        Yields:
            (fold_idx, data_dict) for each fold
        """
        n_folds = self.cfg.cv_n_splits
        val_ratio = getattr(self.cfg, 'val_ratio', 0.1)
        test_ratio = getattr(self.cfg, 'test_ratio', 0.1)
        
        pretty_title(f"Step 3-5 [CV] æ—¶åºæ»šåŠ¨äº¤å‰éªŒè¯ (Folds={n_folds}, Val={val_ratio:.0%}, Test={test_ratio:.0%})")
        
        max_gap_sec = sampling_sec * self.cfg.max_gap_fill_multiplier
        n_total = len(df)
        all_numeric_cols = self.input_cols + [self.target_col]
        raw_clip_cols = self._get_raw_clip_cols(df)
        
        # ä½¿ç”¨ raw_seq_len è®¡ç®— buffer (ä¸å†ä½¿ç”¨ç²’åŒ–å‚æ•°)
        raw_seq_len = int(self.cfg.raw_seq_len)
        forecast_steps = int(self.cfg.forecast_steps)
        
        # scale_param ç”¨äº _internal_parallel_buildï¼Œä¿æŒå…¼å®¹
        scale_param = {"buf_needed": raw_seq_len, "w_rows": 1, "step_rows": 1}
        tgt_w = 1  # ä¸å†ä½¿ç”¨ç²’åŒ–çª—å£
        K = 1      # ä¸å†ä½¿ç”¨ç²’åŒ–åºåˆ—ï¼ŒK=1 è¡¨ç¤ºå•ä¸€æ—¶é—´æ­¥
        total_buf = raw_seq_len + forecast_steps
        
        # gap åº”è¯¥è‡³å°‘è¦†ç›– prediction_offset + forecast_stepsï¼Œé˜²æ­¢æ•°æ®æ³„æ¼
        prediction_offset = int(getattr(self.cfg, "prediction_offset", 0))
        gap_min = prediction_offset + forecast_steps
        gap_rows = gap_min if self.cfg.optimize_gap_size else max(total_buf, gap_min)
        logging.info(
            f"[CV] gap_rows={gap_rows} (prediction_offset={prediction_offset}, forecast_steps={forecast_steps}, total_buf={total_buf})"
        )
        
        # è®¡ç®—å›ºå®šçš„ val å’Œ test å¤§å°ï¼ˆåŸºäºæ€»æ•°æ®é‡ï¼‰
        val_size = int(n_total * val_ratio)
        test_size = int(n_total * test_ratio)
        min_train_ratio = 0.1  # è‡³å°‘ 10% æ•°æ®ç”¨äºè®­ç»ƒ (æ”¯æŒ8æŠ˜CV)
        
        logging.info(f"[CV] æ•°æ®æ€»é‡: {n_total}, Valå¤§å°: {val_size}, Testå¤§å°: {test_size}")
        
        for fold_idx in range(n_folds):
            # è®¡ç®—è¿™ä¸€æŠ˜çš„æ•°æ®èŒƒå›´
            # fold_end ä» 70% é€æ¸å¢åŠ åˆ° 100%
            if n_folds > 1:
                fold_end_ratio = 0.35 + (fold_idx / (n_folds - 1)) * 0.65
            else:
                fold_end_ratio = 1.0
            fold_end = int(n_total * fold_end_ratio)
            
            # æµ‹è¯•é›†ï¼šæœ€å test_size
            test_start = fold_end - test_size
            test_end = fold_end
            
            # éªŒè¯é›†ï¼šæµ‹è¯•é›†ä¹‹å‰çš„ val_size
            val_start = test_start - val_size
            val_end = test_start
            
            # è®­ç»ƒé›†ï¼šéªŒè¯é›†ä¹‹å‰çš„æ‰€æœ‰æ•°æ®
            train_start = 0
            train_end = val_start - gap_rows  # ç•™å‡º gap é˜²æ­¢æ•°æ®æ³„æ¼
            
            # ç¡®ä¿è®­ç»ƒé›†æœ‰è¶³å¤Ÿæ•°æ®
            if train_end < int(n_total * min_train_ratio):
                logging.warning(f"[CV] Fold {fold_idx+1} è®­ç»ƒé›†ä¸è¶³ ({train_end}/{int(n_total * min_train_ratio)}), è·³è¿‡")
                continue
            
            logging.info(f"[CV] Fold {fold_idx+1}: Train[0:{train_end}], Val[{val_start}:{val_end}], Test[{test_start}:{test_end}]")
            
            # åˆ‡åˆ†æ•°æ®
            df_train = df.iloc[train_start:train_end].copy()
            df_val_with_buf = df.iloc[max(0, val_start - total_buf):val_end].copy()
            df_test_with_buf = df.iloc[max(0, test_start - total_buf):test_end].copy()
            
            # è®­ç»ƒé›†é¢„å¤„ç†
            df_train = self._smart_fill_small_gaps(df_train, self.input_cols, "æ—¶é—´", max_gap_sec)
            df_train = self._fill_missing_values(df_train, all_numeric_cols)
            local_clip_limits: Dict[str, Tuple[float, float]] = {}
            for c in raw_clip_cols:
                q_low, q_high = df_train[c].quantile(0.01), df_train[c].quantile(0.99)
                if np.isfinite(q_low) and np.isfinite(q_high) and q_high > q_low:
                    local_clip_limits[c] = (float(q_low), float(q_high))
                    df_train[c] = df_train[c].clip(lower=q_low, upper=q_high)
            df_train = self._final_safety_net(df_train, all_numeric_cols)

            def _clean_fold_df(d_):
                d_ = self._smart_fill_small_gaps(d_, self.input_cols, "æ—¶é—´", max_gap_sec)
                d_ = self._fill_missing_values(d_, all_numeric_cols)
                for c in raw_clip_cols:
                    if c in local_clip_limits:
                        lower, upper = local_clip_limits[c]
                        d_[c] = d_[c].clip(lower=lower, upper=upper)
                d_ = self._final_safety_net(d_, all_numeric_cols)
                return d_

            df_val_with_buf = _clean_fold_df(df_val_with_buf)
            df_test_with_buf = _clean_fold_df(df_test_with_buf)

            X_tr_mtx = self.scaler_core.fit_transform(df_train[self.core_cols].values)
            X_val_buf_mtx = self.scaler_core.transform(df_val_with_buf[self.core_cols].values)
            X_test_buf_mtx = self.scaler_core.transform(df_test_with_buf[self.core_cols].values)
            
            # [ç›®æ ‡å¹³æ»‘]
            smooth_span = getattr(self.cfg, "target_smooth_span", 0)
            
            y_tr_target = df_train[self.target_col].values
            y_tr_raw_cv = y_tr_target.copy()
            if smooth_span > 0:
                y_tr_target = pd.Series(y_tr_target).ewm(span=smooth_span, min_periods=1).mean().values
            
            y_val_target = df_val_with_buf[self.target_col].values
            y_val_raw_cv = y_val_target.copy()
            if smooth_span > 0:
                y_val_target = pd.Series(y_val_target).ewm(span=smooth_span, min_periods=1).mean().values
            
            y_test_target = df_test_with_buf[self.target_col].values
            y_test_raw_cv = y_test_target.copy()
            if smooth_span > 0:
                y_test_target = pd.Series(y_test_target).ewm(span=smooth_span, min_periods=1).mean().values
            
            # æ„å»ºè®­ç»ƒé›†æ ·æœ¬
            X_tr_mac, X_tr_raw, y_tr_norm, anc_tr, y_tr_obs = self._internal_parallel_build(
                total_buf, len(df_train), X_tr_mtx, y_tr_target, df_train["æ—¶é—´"].values,
                scale_param, tgt_w, K, 4 * X_tr_mtx.shape[1], max_gap_sec, sampling_sec, f"F{fold_idx}_Tr",
                y_raw_all=y_tr_raw_cv
            )
            
            self.scaler_y.fit(y_tr_norm)
            
            # æ„å»ºéªŒè¯é›†æ ·æœ¬
            X_val_mac, X_val_raw, y_val_norm, anc_val, y_val_obs = self._internal_parallel_build(
                total_buf + tgt_w, len(df_val_with_buf), X_val_buf_mtx, y_val_target, df_val_with_buf["æ—¶é—´"].values,
                scale_param, tgt_w, K, 4 * X_val_buf_mtx.shape[1], max_gap_sec, sampling_sec, f"F{fold_idx}_Val",
                y_raw_all=y_val_raw_cv
            )
            
            # æ„å»ºæµ‹è¯•é›†æ ·æœ¬
            X_test_mac, X_test_raw, y_test_norm, anc_test, y_test_obs = self._internal_parallel_build(
                total_buf + tgt_w, len(df_test_with_buf), X_test_buf_mtx, y_test_target, df_test_with_buf["æ—¶é—´"].values,
                scale_param, tgt_w, K, 4 * X_test_buf_mtx.shape[1], max_gap_sec, sampling_sec, f"F{fold_idx}_Test",
                y_raw_all=y_test_raw_cv
            )
            
            fold_data = {
                "X_tr": X_tr_mac, "X_tr_raw": X_tr_raw,
                "y_tr": self.scaler_y.transform(y_tr_norm), "anchor_tr": anc_tr,
                "X_val": X_val_mac, "X_val_raw": X_val_raw,
                "y_val": self.scaler_y.transform(y_val_norm), "anchor_val": anc_val,
                "y_val_raw": y_val_norm, "y_val_obs": y_val_obs,
                "X_test": X_test_mac, "X_test_raw": X_test_raw,
                "y_test": self.scaler_y.transform(y_test_norm), "anchor_test": anc_test,
                "y_test_raw": y_test_norm, "y_test_obs": y_test_obs,
                "granulated_feat_dim": X_tr_mac.shape[-1],
                "raw_feat_dim": X_tr_raw.shape[-1]
            }
            yield fold_idx, fold_data


    def process_and_split(self, df: pd.DataFrame, sampling_sec: float) -> Dict:
        pretty_title("Step 3-5  è®­ç»ƒ/éªŒè¯/æµ‹è¯• æ•°æ®åˆ’åˆ†ä¸å¤„ç† (å•å°ºåº¦)")
        
        if getattr(self.cfg, "USE_DATA_CACHE", False):
            cache_path = self.get_cache_path()
            if os.path.exists(cache_path):
                logging.info(f"[Cache] æ£€æµ‹åˆ°ç¼“å­˜ï¼Œæ­£åœ¨åŠ è½½: {cache_path}")
                try:
                    with open(cache_path, 'rb') as f:
                        cache_data = pickle.load(f)
                    # åŒæ­¥é¢„å¤„ç†å™¨çš„çŠ¶æ€
                    if "preprocessor_state" in cache_data:
                        state = cache_data.pop("preprocessor_state")
                        self.scaler_core = state["scaler_core"]
                        self.scaler_aux = state["scaler_aux"]
                        self.pca_aux = state["pca_aux"]
                        self.scaler_y = state["scaler_y"]
                        self.core_cols = state["core_cols"]
                        self.aux_cols = state["aux_cols"]
                        self.input_cols = state["input_cols"]
                    return cache_data
                except Exception as e:
                    logging.warning(f"[Cache] åŠ è½½ç¼“å­˜å¤±è´¥: {e}ï¼Œå°†é‡æ–°ç”Ÿæˆæ•°æ®ã€‚")

        max_gap_sec = sampling_sec * self.cfg.max_gap_fill_multiplier
        n_total_raw = len(df)
        n_test = int(n_total_raw * self.cfg.test_split)
        n_val = int(n_total_raw * self.cfg.val_split)
        n_train = n_total_raw - n_val - n_test
        
        # 1. åŸºç¡€æ•°æ®é›†åˆ‡åˆ†
        df_train = df.iloc[:n_train].copy()
        df_val = df.iloc[n_train : n_train + n_val].copy()
        df_test = df.iloc[n_train + n_val:].copy()
        
        # 2. è®­ç»ƒé›†é¢„å¤„ç†ä¸å½’ä¸€åŒ–æ‹Ÿåˆ
        all_numeric_cols = self.input_cols + [self.target_col]
        raw_clip_cols = self._get_raw_clip_cols(df_train)
        df_train = self._smart_fill_small_gaps(df_train, self.input_cols, "æ—¶é—´", max_gap_sec)
        df_train = self._fill_missing_values(df_train, all_numeric_cols)
        self._compute_clip_limits(df_train, raw_clip_cols)
        df_train = self._apply_clip_limits(df_train, raw_clip_cols)
        df_train = self._final_safety_net(df_train, all_numeric_cols)
        
        self.scaler_core.fit(df_train[self.core_cols].values)
        if len(self.aux_cols) > 0:
            self.scaler_aux.fit(df_train[self.aux_cols].values)
            aux_scaled = self.scaler_aux.transform(df_train[self.aux_cols].values)
            self.pca_aux.fit(aux_scaled)

        def _clean_df(d_):
            d_ = self._smart_fill_small_gaps(d_, self.input_cols, "æ—¶é—´", max_gap_sec)
            d_ = self._fill_missing_values(d_, all_numeric_cols)
            d_ = self._apply_clip_limits(d_, raw_clip_cols)
            d_ = self._final_safety_net(d_, all_numeric_cols)
            return d_

        df_val = _clean_df(df_val)
        df_test = _clean_df(df_test)

        def _transform_to_matrix(d_):
            core_data = self.scaler_core.transform(d_[self.core_cols].values)
            if len(self.aux_cols) > 0:
                aux_data = self.scaler_aux.transform(d_[self.aux_cols].values)
                pca_data = self.pca_aux.transform(aux_data)
                return np.hstack([core_data, pca_data])
            else:
                return core_data

        X_tr_combined = _transform_to_matrix(df_train)
        X_val_combined = _transform_to_matrix(df_val)
        X_test_combined = _transform_to_matrix(df_test)

        X_all = np.vstack([X_tr_combined, X_val_combined, X_test_combined])
        df = self._final_safety_net(df, all_numeric_cols)
        y_raw = df[self.target_col].values.copy()
        # [ç›®æ ‡å¹³æ»‘] å¯¹è®­ç»ƒç›®æ ‡åš EMA å¹³æ»‘ï¼Œè®©æ¨¡å‹å­¦ä¹ é¢„æµ‹å¹³æ»‘ä¿¡å·
        smooth_span = getattr(self.cfg, "target_smooth_span", 0)
        if smooth_span > 0:
            y_all = pd.Series(y_raw).ewm(span=smooth_span, min_periods=1).mean().values
            logging.info(f"[Smooth] ç›®æ ‡ EMA å¹³æ»‘å·²å¯ç”¨, span={smooth_span}")
        else:
            y_all = y_raw.copy()
        time_all = df["æ—¶é—´"].values

        # ä½¿ç”¨ raw_seq_len è®¡ç®— buffer (ä¸å†ä½¿ç”¨ç²’åŒ–å‚æ•°)
        raw_seq_len = int(self.cfg.raw_seq_len)
        forecast_steps = int(self.cfg.forecast_steps)
        
        # scale_param ç”¨äº _internal_parallel_buildï¼Œä¿æŒå…¼å®¹
        scale_param = {"buf_needed": raw_seq_len, "w_rows": 1, "step_rows": 1}
        tgt_w = 1  # ä¸å†ä½¿ç”¨ç²’åŒ–çª—å£
        K = 1      # ä¸å†ä½¿ç”¨ç²’åŒ–åºåˆ—ï¼ŒK=1 è¡¨ç¤ºå•ä¸€æ—¶é—´æ­¥
        total_buf = raw_seq_len + forecast_steps
        
        # gap åº”è¯¥è‡³å°‘è¦†ç›– prediction_offset + forecast_stepsï¼Œé˜²æ­¢æ•°æ®æ³„æ¼
        prediction_offset = int(getattr(self.cfg, "prediction_offset", 0))
        gap_min = prediction_offset + forecast_steps
        gap = gap_min if self.cfg.optimize_gap_size else max(total_buf, gap_min)
        logging.info(
            f"[Split] gap={gap} (prediction_offset={prediction_offset}, forecast_steps={forecast_steps}, total_buf={total_buf})"
        )

        # 4. æ„å»ºæ•°æ® (y_all=å¹³æ»‘ç›®æ ‡ç”¨äºè®­ç»ƒæ ‡ç­¾, y_raw=åŸå§‹å€¼ç”¨äºè§‚æµ‹çª—)
        # [ä¿®å¤] è®­ç»ƒæ ‡ç­¾æ³„éœ²ï¼šä»…å…è®¸æ ‡ç­¾ç´¢å¼•è½åœ¨è®­ç»ƒé›†ä¸Šç•Œå†…
        X_tr_mac, X_tr_raw, y_tr_raw, anc_tr, y_tr_obs = self._internal_parallel_build(
            total_buf, n_train, X_all, y_all, time_all, scale_param, tgt_w, K,
            4 * X_all.shape[1], max_gap_sec, sampling_sec, "Train", y_raw_all=y_raw,
            max_future_index=n_train - 1
        )
        self.scaler_y.fit(y_tr_raw)

        val_start_idx = n_train + gap + total_buf
        X_val_mac, X_val_raw, y_val_raw, anc_val, y_val_obs = self._internal_parallel_build(
            val_start_idx, n_train + n_val, X_all, y_all, time_all, scale_param, tgt_w, K,
            4 * X_all.shape[1], max_gap_sec, sampling_sec, "Val", y_raw_all=y_raw,
            max_future_index=n_train + n_val - 1
        )

        test_start_idx = n_train + n_val + gap + total_buf
        X_test_mac, X_test_raw, y_test_raw, anc_test, y_test_obs = self._internal_parallel_build(
            test_start_idx, len(X_all), X_all, y_all, time_all, scale_param, tgt_w, K,
            4 * X_all.shape[1], max_gap_sec, sampling_sec, "Test", y_raw_all=y_raw,
            max_future_index=len(X_all) - 1
        )

        granulated_feat_dim = X_tr_mac.shape[-1]

        # æå–æµ‹è¯•é›†å¯¹åº”çš„æ—¶é—´æˆ³
        # æµ‹è¯•é›†æ ·æœ¬çš„æ—¶é—´æˆ³å¯¹åº”äºæ¯ä¸ªæ ·æœ¬çš„æœ€åä¸€ä¸ªæ—¶é—´ç‚¹
        test_sample_count = X_test_raw.shape[0]
        test_time_indices = np.arange(test_start_idx, test_start_idx + test_sample_count)
        # ç¡®ä¿ç´¢å¼•ä¸è¶Šç•Œ
        test_time_indices = test_time_indices[test_time_indices < len(time_all)]
        timestamps_test = time_all[test_time_indices] if len(test_time_indices) == test_sample_count else None
        
        result = {
            "X_tr": X_tr_mac, "X_val": X_val_mac, "X_test": X_test_mac,
            "X_tr_raw": X_tr_raw, "X_val_raw": X_val_raw, "X_test_raw": X_test_raw,
            "y_tr": self.scaler_y.transform(y_tr_raw), "anchor_tr": anc_tr,
            "y_val": self.scaler_y.transform(y_val_raw), "anchor_val": anc_val,
            "y_test": self.scaler_y.transform(y_test_raw), "anchor_test": anc_test,
            "y_test_raw": y_test_raw, "y_test_obs": y_test_obs,
            "granulated_feat_dim": granulated_feat_dim,
            "raw_feat_dim": X_tr_raw.shape[-1],
            "timestamps_test": timestamps_test
        }

        if getattr(self.cfg, "USE_DATA_CACHE", False):
            cache_path = self.get_cache_path()
            logging.info(f"[Cache] æ­£åœ¨ä¿å­˜æ•°æ®åˆ°ç¼“å­˜: {cache_path}")
            # ä¿å­˜é¢„å¤„ç†å™¨çŠ¶æ€ï¼Œä»¥ä¾¿åŠ è½½ç¼“å­˜æ—¶èƒ½æ¢å¤
            cache_to_save = result.copy()
            cache_to_save["preprocessor_state"] = {
                "scaler_core": self.scaler_core,
                "scaler_aux": self.scaler_aux,
                "pca_aux": self.pca_aux,
                "scaler_y": self.scaler_y,
                "core_cols": self.core_cols,
                "aux_cols": self.aux_cols,
                "input_cols": self.input_cols
            }
            try:
                with open(cache_path, 'wb') as f:
                    pickle.dump(cache_to_save, f)
            except Exception as e:
                logging.warning(f"[Cache] ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")

        return result
    
    def save(self, save_dir: str):
        os.makedirs(save_dir, exist_ok=True)
        joblib.dump(self.scaler_core, os.path.join(save_dir, "scaler_core.pkl"))
        joblib.dump(self.scaler_aux, os.path.join(save_dir, "scaler_aux.pkl"))
        joblib.dump(self.pca_aux, os.path.join(save_dir, "pca_aux.pkl"))
        joblib.dump(self.scaler_y, os.path.join(save_dir, "scaler_y.pkl"))
        logging.info(f"[Save] é¢„å¤„ç†å™¨å·²ä¿å­˜åˆ° {save_dir}")

    def load(self, save_dir: str):
        self.scaler_core = joblib.load(os.path.join(save_dir, "scaler_core.pkl"))
        self.scaler_aux = joblib.load(os.path.join(save_dir, "scaler_aux.pkl"))
        self.pca_aux = joblib.load(os.path.join(save_dir, "pca_aux.pkl"))
        self.scaler_y = joblib.load(os.path.join(save_dir, "scaler_y.pkl"))
        logging.info(f"[Load] é¢„å¤„ç†å™¨å·²åŠ è½½ from {save_dir}")
